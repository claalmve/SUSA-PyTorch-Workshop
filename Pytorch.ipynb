{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SUSA PyTorch Workshop\n",
    "## 11/28/18\n",
    "\n",
    "### Hosted by and maintained by the [Statistics Undergraduate Students Association (SUSA)](https://susa.berkeley.edu). Originally authored by [Calvin Chen](mailto:chencalvin99@berkeley.edu), [Samyak Parajuli](mailto:samyak.parajuli@berkeley.edu), and [Johann Sun](mailto:sunjohann@berkeley.edu).\n",
    "\n",
    "### Table of Contents\n",
    "* [What is PyTorch](#what_is_pytorch)\n",
    "    * [Installing PyTorch](#installing_pytorch)\n",
    "    * [Advantages of PyTorch](#advantages_of_pytorch)\n",
    "* [Similarities to Numpy](#similarities_to_numpy)\n",
    "* [GPU](#gpu)\n",
    "* [Classification vs Regression](#class_vs_regress)\n",
    "    * [Recap on Regression](#regress_recap)\n",
    "    * [Some Intuition on Classification](#intuition)\n",
    "        * [Real World Example](#real_world_example)\n",
    "* [k-Nearest Neighbors](#knn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='what_is_pytorch'></a>\n",
    "# What is PyTorch?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch is a library made for the Python programming language as a platform on which to develop deep learning projects and applications. Released in Januuary 2016, it has gained fame for being able to create neural networks with relative ease. Before diving directly into PyTorch, it's important to understand the definition of a deep learning platform, deep learning, neural networks, and how they're connected. In a general sense, Deep Learning is a subset of Machine Learning, which is a subset of the large sphere of Artifical Intelligence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-27T05:13:48.987690Z",
     "start_time": "2018-11-27T05:13:48.852623Z"
    },
    "collapsed": true
   },
   "source": [
    "![title](img/ai_subsets.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, PyTorch would lie somewhere in the most inner circle of the graphic above, as a platform for people to develop Deep Learning models off of. \n",
    "\n",
    "Now, you might be interested in different applications of Neural Networks, and here's a couple that we found that were pretty cool!\n",
    "\n",
    "[Celebrity Faces](https://www.youtube.com/watch?v=36lE9tV9vm0)\n",
    "\n",
    "[Atari Breakout](https://www.youtube.com/watch?v=V1eYniJ0Rnk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To kick things off, we'll begin with downloading PyTorch and getting into the differences between it and other libraries that exist out there!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='installing_pytorch'></a>\n",
    "## Installing Pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please install the correct version of Pytorch corresponding to your OS here: [PyTorch](https://pytorch.org/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you've got PyTorch downloaded, let's get into why it's useful, and how it differs from other platforms/packages that do similar things to it out there!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='advantages_of_pytorch'></a>\n",
    "## Advantages of PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simple Interface: It offers easy to use API, thus it is very simple to operate and run like Python.\n",
    "Pythonic in nature: This library, being Pythonic, smoothly integrates with the Python data science stack. Thus it can leverage all the services and functionalities offered by the Python environment.\n",
    "Computational graphs: In addition to this, PyTorch provides an excellent platform which offers dynamic computational graphs, thus you can change them during runtime. This is highly useful when you have no idea how much memory will be required for creating a neural network model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So what are the features of PyTorch that makes it so great? Here's a few of the main advantages that PyTorch holds over other Deep Learning platforms (like TensorFlow):\n",
    "* **Built specifically for Python**\n",
    "    * Integrates really smoothly with different Python packages (data visualization)\n",
    "    * Good debugging environment- can just introduce different debugging packages in Python and they integrate smoothly\n",
    "* **Imperative**\n",
    "    * Uses statements to continuously change the program's state\n",
    "    * Does not require for frameworks of code to be developed before being execute; executes on the fly\n",
    "* **Dynamic Computation Graph**\n",
    "    * Defined and manipulated as you code, as compared to static, where you define the graph before you run the model\n",
    "    * Really useful with dynamic inputs- static graphs can only have static input sequences!\n",
    "        * For RNNs, it's incredibly useful to have a dynamic computation graph, as inputs might change in structure over time (we'll get into more of RNNs later and why this is useful!)\n",
    "* **Declarative Data Parallelism**\n",
    "    * Easy to call torch.nn.DataParallel to wrap different modules and split samples into subsets to run computation for these subsets in parallel (provides for better efficiency and speed, and minimal effort to set up)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've been able to see the advantages/specifics of PyTorch from a high level, let's take a look at its implementation in code and how it compares with another famous package in Python- Numpy!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='similarities_to_numpy'></a>\n",
    "# Similarities to Numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we can get into any of the coding, we'll need to define what a tensor is in its relation to Deep Learning and coding!\n",
    "\n",
    "To define it generally, a tensor is a basically just an $n$ dimensional object (scalar, vector, matrix). What this means is that a rank 0 tensor can be represented with a 0-dimensional object (a scalar), a rank 1 tensor can be represented with a 1-dimensional object (vector), and a rank 2 tensor can be represented with a 2-dimensional object (matrix). **However, this does not mean that these objects can be represented as tensors!** Tensors have special properties that make them useful in Deep Learning, one being that they're flexible in structure and can take on any arbitrary rank.\n",
    "\n",
    "Another implication with tensors that make them more useful than just vectors or matrices in Deep Learning models are how they can be **variable with relation to the other entities around them**. For example, if you wanted a model to maintain consistent weights despite the fact that different inputs may be transformed, tensors would be able to be replaced with the scalar weights to be able to sustain different transformations while not impacting the outputs!\n",
    "\n",
    "Now, these definitions/usages for tensors may be confusing, but we just wanted to give you a brief glimpse into why it's more useful than just using objects with static dimensions. For the purposes of this notebook, it'll be sufficient to just interpret tensors as \"arrays\", but just know that there are key differences between the two. If you're interested in learning more about the differences between the two, click [here](https://medium.com/@quantumsteinke/whats-the-difference-between-a-matrix-and-a-tensor-4505fbdc576c) to learn more!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But enough with all the definitions and English- let's get into coding!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-27T05:18:20.062179Z",
     "start_time": "2018-11-27T05:18:19.621325Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-27T05:18:20.096057Z",
     "start_time": "2018-11-27T05:18:20.064643Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 0., 1.]])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.eye(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-27T05:18:20.550881Z",
     "start_time": "2018-11-27T05:18:20.545485Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [0., 0., 1.]])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.eye(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-27T05:18:59.504281Z",
     "start_time": "2018-11-27T05:18:59.497933Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.96498995, 0.26053359, 0.53001272],\n",
       "       [0.56542759, 0.21948031, 0.10098997],\n",
       "       [0.66028063, 0.34336957, 0.58764673],\n",
       "       [0.49277861, 0.32095331, 0.42830881],\n",
       "       [0.37064033, 0.62725371, 0.19902255]])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.rand(5,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-27T05:19:00.462125Z",
     "start_time": "2018-11-27T05:19:00.438903Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0542, 0.2257, 0.3019],\n",
       "        [0.3522, 0.7485, 0.2319],\n",
       "        [0.5271, 0.4625, 0.5110],\n",
       "        [0.9013, 0.1109, 0.3437],\n",
       "        [0.6411, 0.3683, 0.0146]])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.rand((5, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-27T05:21:02.920036Z",
     "start_time": "2018-11-27T05:21:02.914543Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2., 1., 1.],\n",
       "        [1., 2., 1.],\n",
       "        [1., 1., 2.]])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = torch.eye(3)\n",
    "A.add(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[3., 2., 2.],\n",
       "        [2., 3., 2.],\n",
       "        [2., 2., 3.]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-27T05:22:23.494698Z",
     "start_time": "2018-11-27T05:22:23.488406Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2., 2.],\n",
       "        [2., 2.]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1 = torch.ones(2, 2)\n",
    "t2 = torch.ones(2, 2)\n",
    "t = t1 + t2\n",
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-27T05:22:23.764738Z",
     "start_time": "2018-11-27T05:22:23.757874Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2., 1., 1.],\n",
       "        [1., 2., 1.],\n",
       "        [1., 1., 2.]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = torch.eye(3)\n",
    "A.add(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-27T05:22:24.444486Z",
     "start_time": "2018-11-27T05:22:24.439872Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [0., 0., 1.]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Underscore means inplace operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[3., 2., 2.],\n",
       "        [2., 3., 2.],\n",
       "        [2., 2., 3.]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A.add_(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[3., 2., 2.],\n",
       "        [2., 3., 2.],\n",
       "        [2., 2., 3.]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Converting from Pytorch to Numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converting from numpy to torch, allows us to build upon existing architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [0., 0., 1.]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = torch.eye(3)\n",
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 0., 1.]], dtype=float32)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [0., 0., 1.]], dtype=torch.float64)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.from_numpy(np.eye(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='gpu'></a>\n",
    "# GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GPUs have many processing units but limited memory. Also, they can only use data in their own memory, not in the CPU's memory, so one must transfer data back \n",
    "and forth between the CPU and the GPU. This copying can, in some computations, constitute a very large fraction of the overall \n",
    "computation. So it is best to create the data and/or leave the data (for subsequent calculations) on the GPU when possible and to limit transfers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#If you have a GPU you should get something like:  device(type='cuda', index=0)\n",
    "#You can move data to the GPU by doing .to(device)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [0., 0., 1.]])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = torch.eye(3)\n",
    "data.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='liner_regression_example'></a>\n",
    "# Linear Regression example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_regression\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "n_features = 1\n",
    "n_samples = 100\n",
    "\n",
    "X, y = make_regression(\n",
    "    n_samples=n_samples,\n",
    "    n_features=n_features,\n",
    "    noise=10,\n",
    ")\n",
    "\n",
    "fix, ax = plt.subplots()\n",
    "ax.plot(X, y, \".\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = 1  # a random guess: random value\n",
    "\n",
    "# our model forward pass\n",
    "\n",
    "def forward(x):\n",
    "    return x * w\n",
    "\n",
    "# Loss function\n",
    "def loss(x, y):\n",
    "    y_pred = forward(x)\n",
    "    return (y_pred - y) ** 2\n",
    "\n",
    "# compute gradient\n",
    "def gradient(x, y):  # d_loss/d_w\n",
    "    return 2 * x * (x * w - y)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(10):\n",
    "    for x_val, y_val in zip(X, y):\n",
    "        grad = gradient(x_val, y_val)\n",
    "        w = w - 0.01 * grad\n",
    "        #print(\"\\tgrad: \", X, y, np.round(grad, 2))\n",
    "        l = loss(x_val, y_val)\n",
    "\n",
    "# After training\n",
    "y_ = forward(X)\n",
    "#print(y_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Vis\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(X, y_, \".\", label=\"pred\")\n",
    "ax.plot(X, y, \".\", label=\"data\")\n",
    "ax.set_title(\"MSE: {loss.item():0.1f}\")\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='auto_grad'></a>\n",
    "# Autograd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Every Tensor has a flag: requires_grad that allows for fine grained exclusion of subgraphs from gradient computation and can increase efficiency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-27T05:26:46.858149Z",
     "start_time": "2018-11-27T05:26:46.853092Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w = torch.ones(1)\n",
    "w.requires_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Tensor"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = torch.ones(1) * 2\n",
    "z.requires_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3.])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total = w + z\n",
    "total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "element 0 of tensors does not require grad and does not have a grad_fn",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-fb606bf656ee>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtotal\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m     91\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \"\"\"\n\u001b[0;32m---> 93\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     88\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     89\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: element 0 of tensors does not require grad and does not have a grad_fn"
     ]
    }
   ],
   "source": [
    "total.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = torch.ones(1, requires_grad=True)\n",
    "w.requires_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total = w**2 + z\n",
    "total.requires_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(w.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    total = w + z\n",
    "\n",
    "total.requires_grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='linear_regression_pytorch'></a>\n",
    "# Linear Regression (Pytorch Way)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.from_numpy(X).float()\n",
    "y = torch.from_numpy(y.reshape((n_samples, n_features))).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from torch import optim\n",
    "\n",
    "class LinReg(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super().__init__()\n",
    "        self.beta = nn.Linear(input_dim, 1)\n",
    "        self.beta.weight.data.fill_(1)\n",
    "\n",
    "        \n",
    "    def forward(self, X):\n",
    "        return self.beta(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = LinReg(n_features).to(device)  # <-- Calculations will happen on the GPU\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "X, y = X.to(device), y.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train step\n",
    "for _ in range(10):\n",
    "    model.train() # Puts model in train mode\n",
    "    optimizer.zero_grad() # Need to null the gradient, otherwise it will accumulate over time\n",
    "\n",
    "    y_ = model(X) # Pass data into model and get prediction\n",
    "    loss = criterion(y_, y) # calculate mse loss\n",
    "    loss.backward() # Calculate gradients\n",
    "    optimizer.step() # Update the parameter\n",
    "\n",
    "# Eval\n",
    "model.eval()\n",
    "with torch.no_grad(): #<- Not interested in gradients\n",
    "    y_ = model(X)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Vis\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(X.cpu().numpy(), y_.cpu().numpy(), \".\", label=\"pred\")\n",
    "ax.plot(X.cpu().numpy(), y.cpu().numpy(), \".\", label=\"data\")\n",
    "ax.set_title(\"MSE: {loss.item():0.1f}\")\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='neural_network'></a>\n",
    "# Neural Network Implementation with MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Processing...\n",
      "Done!\n",
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.303662\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/calvin/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:69: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [640/60000 (1%)]\tLoss: 2.303379\n",
      "Train Epoch: 1 [1280/60000 (2%)]\tLoss: 2.298976\n",
      "Train Epoch: 1 [1920/60000 (3%)]\tLoss: 2.302488\n",
      "Train Epoch: 1 [2560/60000 (4%)]\tLoss: 2.301148\n",
      "Train Epoch: 1 [3200/60000 (5%)]\tLoss: 2.306086\n",
      "Train Epoch: 1 [3840/60000 (6%)]\tLoss: 2.307215\n",
      "Train Epoch: 1 [4480/60000 (7%)]\tLoss: 2.300955\n",
      "Train Epoch: 1 [5120/60000 (9%)]\tLoss: 2.310881\n",
      "Train Epoch: 1 [5760/60000 (10%)]\tLoss: 2.293291\n",
      "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 2.290688\n",
      "Train Epoch: 1 [7040/60000 (12%)]\tLoss: 2.319289\n",
      "Train Epoch: 1 [7680/60000 (13%)]\tLoss: 2.303837\n",
      "Train Epoch: 1 [8320/60000 (14%)]\tLoss: 2.302586\n",
      "Train Epoch: 1 [8960/60000 (15%)]\tLoss: 2.308828\n",
      "Train Epoch: 1 [9600/60000 (16%)]\tLoss: 2.305709\n",
      "Train Epoch: 1 [10240/60000 (17%)]\tLoss: 2.307690\n",
      "Train Epoch: 1 [10880/60000 (18%)]\tLoss: 2.283756\n",
      "Train Epoch: 1 [11520/60000 (19%)]\tLoss: 2.300191\n",
      "Train Epoch: 1 [12160/60000 (20%)]\tLoss: 2.300406\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 2.284857\n",
      "Train Epoch: 1 [13440/60000 (22%)]\tLoss: 2.295779\n",
      "Train Epoch: 1 [14080/60000 (23%)]\tLoss: 2.305052\n",
      "Train Epoch: 1 [14720/60000 (25%)]\tLoss: 2.290446\n",
      "Train Epoch: 1 [15360/60000 (26%)]\tLoss: 2.299261\n",
      "Train Epoch: 1 [16000/60000 (27%)]\tLoss: 2.293176\n",
      "Train Epoch: 1 [16640/60000 (28%)]\tLoss: 2.290493\n",
      "Train Epoch: 1 [17280/60000 (29%)]\tLoss: 2.286940\n",
      "Train Epoch: 1 [17920/60000 (30%)]\tLoss: 2.294491\n",
      "Train Epoch: 1 [18560/60000 (31%)]\tLoss: 2.292125\n",
      "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 2.287814\n",
      "Train Epoch: 1 [19840/60000 (33%)]\tLoss: 2.290705\n",
      "Train Epoch: 1 [20480/60000 (34%)]\tLoss: 2.297337\n",
      "Train Epoch: 1 [21120/60000 (35%)]\tLoss: 2.296347\n",
      "Train Epoch: 1 [21760/60000 (36%)]\tLoss: 2.288836\n",
      "Train Epoch: 1 [22400/60000 (37%)]\tLoss: 2.275997\n",
      "Train Epoch: 1 [23040/60000 (38%)]\tLoss: 2.290298\n",
      "Train Epoch: 1 [23680/60000 (39%)]\tLoss: 2.292607\n",
      "Train Epoch: 1 [24320/60000 (41%)]\tLoss: 2.287780\n",
      "Train Epoch: 1 [24960/60000 (42%)]\tLoss: 2.278872\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 2.282593\n",
      "Train Epoch: 1 [26240/60000 (44%)]\tLoss: 2.285764\n",
      "Train Epoch: 1 [26880/60000 (45%)]\tLoss: 2.280637\n",
      "Train Epoch: 1 [27520/60000 (46%)]\tLoss: 2.282995\n",
      "Train Epoch: 1 [28160/60000 (47%)]\tLoss: 2.273337\n",
      "Train Epoch: 1 [28800/60000 (48%)]\tLoss: 2.281275\n",
      "Train Epoch: 1 [29440/60000 (49%)]\tLoss: 2.280265\n",
      "Train Epoch: 1 [30080/60000 (50%)]\tLoss: 2.288963\n",
      "Train Epoch: 1 [30720/60000 (51%)]\tLoss: 2.279900\n",
      "Train Epoch: 1 [31360/60000 (52%)]\tLoss: 2.278026\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 2.276153\n",
      "Train Epoch: 1 [32640/60000 (54%)]\tLoss: 2.273198\n",
      "Train Epoch: 1 [33280/60000 (55%)]\tLoss: 2.267993\n",
      "Train Epoch: 1 [33920/60000 (57%)]\tLoss: 2.272086\n",
      "Train Epoch: 1 [34560/60000 (58%)]\tLoss: 2.264558\n",
      "Train Epoch: 1 [35200/60000 (59%)]\tLoss: 2.261421\n",
      "Train Epoch: 1 [35840/60000 (60%)]\tLoss: 2.258605\n",
      "Train Epoch: 1 [36480/60000 (61%)]\tLoss: 2.255226\n",
      "Train Epoch: 1 [37120/60000 (62%)]\tLoss: 2.270460\n",
      "Train Epoch: 1 [37760/60000 (63%)]\tLoss: 2.259447\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 2.261926\n",
      "Train Epoch: 1 [39040/60000 (65%)]\tLoss: 2.260115\n",
      "Train Epoch: 1 [39680/60000 (66%)]\tLoss: 2.239270\n",
      "Train Epoch: 1 [40320/60000 (67%)]\tLoss: 2.248965\n",
      "Train Epoch: 1 [40960/60000 (68%)]\tLoss: 2.225806\n",
      "Train Epoch: 1 [41600/60000 (69%)]\tLoss: 2.240747\n",
      "Train Epoch: 1 [42240/60000 (70%)]\tLoss: 2.211023\n",
      "Train Epoch: 1 [42880/60000 (71%)]\tLoss: 2.211748\n",
      "Train Epoch: 1 [43520/60000 (72%)]\tLoss: 2.210394\n",
      "Train Epoch: 1 [44160/60000 (74%)]\tLoss: 2.191445\n",
      "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 2.187142\n",
      "Train Epoch: 1 [45440/60000 (76%)]\tLoss: 2.167234\n",
      "Train Epoch: 1 [46080/60000 (77%)]\tLoss: 2.151489\n",
      "Train Epoch: 1 [46720/60000 (78%)]\tLoss: 2.177588\n",
      "Train Epoch: 1 [47360/60000 (79%)]\tLoss: 2.195252\n",
      "Train Epoch: 1 [48000/60000 (80%)]\tLoss: 2.149153\n",
      "Train Epoch: 1 [48640/60000 (81%)]\tLoss: 2.141044\n",
      "Train Epoch: 1 [49280/60000 (82%)]\tLoss: 2.108296\n",
      "Train Epoch: 1 [49920/60000 (83%)]\tLoss: 2.079626\n",
      "Train Epoch: 1 [50560/60000 (84%)]\tLoss: 2.075534\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 2.102612\n",
      "Train Epoch: 1 [51840/60000 (86%)]\tLoss: 2.018664\n",
      "Train Epoch: 1 [52480/60000 (87%)]\tLoss: 1.956069\n",
      "Train Epoch: 1 [53120/60000 (88%)]\tLoss: 1.947575\n",
      "Train Epoch: 1 [53760/60000 (90%)]\tLoss: 1.890967\n",
      "Train Epoch: 1 [54400/60000 (91%)]\tLoss: 1.841105\n",
      "Train Epoch: 1 [55040/60000 (92%)]\tLoss: 1.833955\n",
      "Train Epoch: 1 [55680/60000 (93%)]\tLoss: 1.855979\n",
      "Train Epoch: 1 [56320/60000 (94%)]\tLoss: 1.852159\n",
      "Train Epoch: 1 [56960/60000 (95%)]\tLoss: 1.666714\n",
      "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 1.705461\n",
      "Train Epoch: 1 [58240/60000 (97%)]\tLoss: 1.561478\n",
      "Train Epoch: 1 [58880/60000 (98%)]\tLoss: 1.564583\n",
      "Train Epoch: 1 [59520/60000 (99%)]\tLoss: 1.519873\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/calvin/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:77: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "/Users/calvin/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:80: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.0231, Accuracy: 5073/10000 (50%)\n",
      "\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 1.474696\n",
      "Train Epoch: 2 [640/60000 (1%)]\tLoss: 1.392233\n",
      "Train Epoch: 2 [1280/60000 (2%)]\tLoss: 1.454554\n",
      "Train Epoch: 2 [1920/60000 (3%)]\tLoss: 1.255085\n",
      "Train Epoch: 2 [2560/60000 (4%)]\tLoss: 1.281534\n",
      "Train Epoch: 2 [3200/60000 (5%)]\tLoss: 1.360458\n",
      "Train Epoch: 2 [3840/60000 (6%)]\tLoss: 1.215719\n",
      "Train Epoch: 2 [4480/60000 (7%)]\tLoss: 1.149964\n",
      "Train Epoch: 2 [5120/60000 (9%)]\tLoss: 1.196114\n",
      "Train Epoch: 2 [5760/60000 (10%)]\tLoss: 1.048898\n",
      "Train Epoch: 2 [6400/60000 (11%)]\tLoss: 1.227630\n",
      "Train Epoch: 2 [7040/60000 (12%)]\tLoss: 1.057390\n",
      "Train Epoch: 2 [7680/60000 (13%)]\tLoss: 1.021081\n",
      "Train Epoch: 2 [8320/60000 (14%)]\tLoss: 0.983530\n",
      "Train Epoch: 2 [8960/60000 (15%)]\tLoss: 0.965556\n",
      "Train Epoch: 2 [9600/60000 (16%)]\tLoss: 1.021137\n",
      "Train Epoch: 2 [10240/60000 (17%)]\tLoss: 1.058683\n",
      "Train Epoch: 2 [10880/60000 (18%)]\tLoss: 0.848543\n",
      "Train Epoch: 2 [11520/60000 (19%)]\tLoss: 0.990776\n",
      "Train Epoch: 2 [12160/60000 (20%)]\tLoss: 0.789379\n",
      "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 0.735964\n",
      "Train Epoch: 2 [13440/60000 (22%)]\tLoss: 0.915260\n",
      "Train Epoch: 2 [14080/60000 (23%)]\tLoss: 0.928270\n",
      "Train Epoch: 2 [14720/60000 (25%)]\tLoss: 0.709915\n",
      "Train Epoch: 2 [15360/60000 (26%)]\tLoss: 0.774893\n",
      "Train Epoch: 2 [16000/60000 (27%)]\tLoss: 0.732084\n",
      "Train Epoch: 2 [16640/60000 (28%)]\tLoss: 0.808970\n",
      "Train Epoch: 2 [17280/60000 (29%)]\tLoss: 0.603320\n",
      "Train Epoch: 2 [17920/60000 (30%)]\tLoss: 0.677046\n",
      "Train Epoch: 2 [18560/60000 (31%)]\tLoss: 0.647688\n",
      "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 0.605112\n",
      "Train Epoch: 2 [19840/60000 (33%)]\tLoss: 0.656732\n",
      "Train Epoch: 2 [20480/60000 (34%)]\tLoss: 0.553743\n",
      "Train Epoch: 2 [21120/60000 (35%)]\tLoss: 0.883675\n",
      "Train Epoch: 2 [21760/60000 (36%)]\tLoss: 0.449633\n",
      "Train Epoch: 2 [22400/60000 (37%)]\tLoss: 0.559687\n",
      "Train Epoch: 2 [23040/60000 (38%)]\tLoss: 0.632501\n",
      "Train Epoch: 2 [23680/60000 (39%)]\tLoss: 0.454299\n",
      "Train Epoch: 2 [24320/60000 (41%)]\tLoss: 0.569083\n",
      "Train Epoch: 2 [24960/60000 (42%)]\tLoss: 0.556335\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.729582\n",
      "Train Epoch: 2 [26240/60000 (44%)]\tLoss: 0.483217\n",
      "Train Epoch: 2 [26880/60000 (45%)]\tLoss: 0.500009\n",
      "Train Epoch: 2 [27520/60000 (46%)]\tLoss: 0.684321\n",
      "Train Epoch: 2 [28160/60000 (47%)]\tLoss: 0.538008\n",
      "Train Epoch: 2 [28800/60000 (48%)]\tLoss: 0.495865\n",
      "Train Epoch: 2 [29440/60000 (49%)]\tLoss: 0.579792\n",
      "Train Epoch: 2 [30080/60000 (50%)]\tLoss: 0.591760\n",
      "Train Epoch: 2 [30720/60000 (51%)]\tLoss: 0.464775\n",
      "Train Epoch: 2 [31360/60000 (52%)]\tLoss: 0.426096\n",
      "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 0.573628\n",
      "Train Epoch: 2 [32640/60000 (54%)]\tLoss: 0.370826\n",
      "Train Epoch: 2 [33280/60000 (55%)]\tLoss: 0.574828\n",
      "Train Epoch: 2 [33920/60000 (57%)]\tLoss: 0.345405\n",
      "Train Epoch: 2 [34560/60000 (58%)]\tLoss: 0.400196\n",
      "Train Epoch: 2 [35200/60000 (59%)]\tLoss: 0.710538\n",
      "Train Epoch: 2 [35840/60000 (60%)]\tLoss: 0.517627\n",
      "Train Epoch: 2 [36480/60000 (61%)]\tLoss: 0.528974\n",
      "Train Epoch: 2 [37120/60000 (62%)]\tLoss: 0.438115\n",
      "Train Epoch: 2 [37760/60000 (63%)]\tLoss: 0.584754\n",
      "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 0.565198\n",
      "Train Epoch: 2 [39040/60000 (65%)]\tLoss: 0.509364\n",
      "Train Epoch: 2 [39680/60000 (66%)]\tLoss: 0.457540\n",
      "Train Epoch: 2 [40320/60000 (67%)]\tLoss: 0.694908\n",
      "Train Epoch: 2 [40960/60000 (68%)]\tLoss: 0.509702\n",
      "Train Epoch: 2 [41600/60000 (69%)]\tLoss: 0.449565\n",
      "Train Epoch: 2 [42240/60000 (70%)]\tLoss: 0.464689\n",
      "Train Epoch: 2 [42880/60000 (71%)]\tLoss: 0.516181\n",
      "Train Epoch: 2 [43520/60000 (72%)]\tLoss: 0.499008\n",
      "Train Epoch: 2 [44160/60000 (74%)]\tLoss: 0.508063\n",
      "Train Epoch: 2 [44800/60000 (75%)]\tLoss: 0.553316\n",
      "Train Epoch: 2 [45440/60000 (76%)]\tLoss: 0.404679\n",
      "Train Epoch: 2 [46080/60000 (77%)]\tLoss: 0.526494\n",
      "Train Epoch: 2 [46720/60000 (78%)]\tLoss: 0.402758\n",
      "Train Epoch: 2 [47360/60000 (79%)]\tLoss: 0.492427\n",
      "Train Epoch: 2 [48000/60000 (80%)]\tLoss: 0.512264\n",
      "Train Epoch: 2 [48640/60000 (81%)]\tLoss: 0.585519\n",
      "Train Epoch: 2 [49280/60000 (82%)]\tLoss: 0.581811\n",
      "Train Epoch: 2 [49920/60000 (83%)]\tLoss: 0.419146\n",
      "Train Epoch: 2 [50560/60000 (84%)]\tLoss: 0.487657\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.425226\n",
      "Train Epoch: 2 [51840/60000 (86%)]\tLoss: 0.345269\n",
      "Train Epoch: 2 [52480/60000 (87%)]\tLoss: 0.536566\n",
      "Train Epoch: 2 [53120/60000 (88%)]\tLoss: 0.424632\n",
      "Train Epoch: 2 [53760/60000 (90%)]\tLoss: 0.420383\n",
      "Train Epoch: 2 [54400/60000 (91%)]\tLoss: 0.399788\n",
      "Train Epoch: 2 [55040/60000 (92%)]\tLoss: 0.684460\n",
      "Train Epoch: 2 [55680/60000 (93%)]\tLoss: 0.452522\n",
      "Train Epoch: 2 [56320/60000 (94%)]\tLoss: 0.630932\n",
      "Train Epoch: 2 [56960/60000 (95%)]\tLoss: 0.497090\n",
      "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 0.460661\n",
      "Train Epoch: 2 [58240/60000 (97%)]\tLoss: 0.385748\n",
      "Train Epoch: 2 [58880/60000 (98%)]\tLoss: 0.513799\n",
      "Train Epoch: 2 [59520/60000 (99%)]\tLoss: 0.634783\n",
      "\n",
      "Test set: Average loss: 0.0063, Accuracy: 8837/10000 (88%)\n",
      "\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.321188\n",
      "Train Epoch: 3 [640/60000 (1%)]\tLoss: 0.279333\n",
      "Train Epoch: 3 [1280/60000 (2%)]\tLoss: 0.646741\n",
      "Train Epoch: 3 [1920/60000 (3%)]\tLoss: 0.280852\n",
      "Train Epoch: 3 [2560/60000 (4%)]\tLoss: 0.632288\n",
      "Train Epoch: 3 [3200/60000 (5%)]\tLoss: 0.303530\n",
      "Train Epoch: 3 [3840/60000 (6%)]\tLoss: 0.437881\n",
      "Train Epoch: 3 [4480/60000 (7%)]\tLoss: 0.462444\n",
      "Train Epoch: 3 [5120/60000 (9%)]\tLoss: 0.361715\n",
      "Train Epoch: 3 [5760/60000 (10%)]\tLoss: 0.538997\n",
      "Train Epoch: 3 [6400/60000 (11%)]\tLoss: 0.310557\n",
      "Train Epoch: 3 [7040/60000 (12%)]\tLoss: 0.456090\n",
      "Train Epoch: 3 [7680/60000 (13%)]\tLoss: 0.352520\n",
      "Train Epoch: 3 [8320/60000 (14%)]\tLoss: 0.461141\n",
      "Train Epoch: 3 [8960/60000 (15%)]\tLoss: 0.623403\n",
      "Train Epoch: 3 [9600/60000 (16%)]\tLoss: 0.387394\n",
      "Train Epoch: 3 [10240/60000 (17%)]\tLoss: 0.446460\n",
      "Train Epoch: 3 [10880/60000 (18%)]\tLoss: 0.424651\n",
      "Train Epoch: 3 [11520/60000 (19%)]\tLoss: 0.349521\n",
      "Train Epoch: 3 [12160/60000 (20%)]\tLoss: 0.490834\n",
      "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 0.399550\n",
      "Train Epoch: 3 [13440/60000 (22%)]\tLoss: 0.526997\n",
      "Train Epoch: 3 [14080/60000 (23%)]\tLoss: 0.355477\n",
      "Train Epoch: 3 [14720/60000 (25%)]\tLoss: 0.375524\n",
      "Train Epoch: 3 [15360/60000 (26%)]\tLoss: 0.289028\n",
      "Train Epoch: 3 [16000/60000 (27%)]\tLoss: 0.353543\n",
      "Train Epoch: 3 [16640/60000 (28%)]\tLoss: 0.497191\n",
      "Train Epoch: 3 [17280/60000 (29%)]\tLoss: 0.416852\n",
      "Train Epoch: 3 [17920/60000 (30%)]\tLoss: 0.404223\n",
      "Train Epoch: 3 [18560/60000 (31%)]\tLoss: 0.426276\n",
      "Train Epoch: 3 [19200/60000 (32%)]\tLoss: 0.401088\n",
      "Train Epoch: 3 [19840/60000 (33%)]\tLoss: 0.386063\n",
      "Train Epoch: 3 [20480/60000 (34%)]\tLoss: 0.362328\n",
      "Train Epoch: 3 [21120/60000 (35%)]\tLoss: 0.286817\n",
      "Train Epoch: 3 [21760/60000 (36%)]\tLoss: 0.555056\n",
      "Train Epoch: 3 [22400/60000 (37%)]\tLoss: 0.290122\n",
      "Train Epoch: 3 [23040/60000 (38%)]\tLoss: 0.471133\n",
      "Train Epoch: 3 [23680/60000 (39%)]\tLoss: 0.482666\n",
      "Train Epoch: 3 [24320/60000 (41%)]\tLoss: 0.350282\n",
      "Train Epoch: 3 [24960/60000 (42%)]\tLoss: 0.233263\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 0.195356\n",
      "Train Epoch: 3 [26240/60000 (44%)]\tLoss: 0.241080\n",
      "Train Epoch: 3 [26880/60000 (45%)]\tLoss: 0.622767\n",
      "Train Epoch: 3 [27520/60000 (46%)]\tLoss: 0.338434\n",
      "Train Epoch: 3 [28160/60000 (47%)]\tLoss: 0.264347\n",
      "Train Epoch: 3 [28800/60000 (48%)]\tLoss: 0.442392\n",
      "Train Epoch: 3 [29440/60000 (49%)]\tLoss: 0.249364\n",
      "Train Epoch: 3 [30080/60000 (50%)]\tLoss: 0.611429\n",
      "Train Epoch: 3 [30720/60000 (51%)]\tLoss: 0.311797\n",
      "Train Epoch: 3 [31360/60000 (52%)]\tLoss: 0.456721\n",
      "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 0.462888\n",
      "Train Epoch: 3 [32640/60000 (54%)]\tLoss: 0.175290\n",
      "Train Epoch: 3 [33280/60000 (55%)]\tLoss: 0.313085\n",
      "Train Epoch: 3 [33920/60000 (57%)]\tLoss: 0.560535\n",
      "Train Epoch: 3 [34560/60000 (58%)]\tLoss: 0.411281\n",
      "Train Epoch: 3 [35200/60000 (59%)]\tLoss: 0.365840\n",
      "Train Epoch: 3 [35840/60000 (60%)]\tLoss: 0.409404\n",
      "Train Epoch: 3 [36480/60000 (61%)]\tLoss: 0.255307\n",
      "Train Epoch: 3 [37120/60000 (62%)]\tLoss: 0.259891\n",
      "Train Epoch: 3 [37760/60000 (63%)]\tLoss: 0.349911\n",
      "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 0.456414\n",
      "Train Epoch: 3 [39040/60000 (65%)]\tLoss: 0.431676\n",
      "Train Epoch: 3 [39680/60000 (66%)]\tLoss: 0.484725\n",
      "Train Epoch: 3 [40320/60000 (67%)]\tLoss: 0.215044\n",
      "Train Epoch: 3 [40960/60000 (68%)]\tLoss: 0.315585\n",
      "Train Epoch: 3 [41600/60000 (69%)]\tLoss: 0.168652\n",
      "Train Epoch: 3 [42240/60000 (70%)]\tLoss: 0.297387\n",
      "Train Epoch: 3 [42880/60000 (71%)]\tLoss: 0.302785\n",
      "Train Epoch: 3 [43520/60000 (72%)]\tLoss: 0.373063\n",
      "Train Epoch: 3 [44160/60000 (74%)]\tLoss: 0.287185\n",
      "Train Epoch: 3 [44800/60000 (75%)]\tLoss: 0.302345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 3 [45440/60000 (76%)]\tLoss: 0.511405\n",
      "Train Epoch: 3 [46080/60000 (77%)]\tLoss: 0.223189\n",
      "Train Epoch: 3 [46720/60000 (78%)]\tLoss: 0.701777\n",
      "Train Epoch: 3 [47360/60000 (79%)]\tLoss: 0.528075\n",
      "Train Epoch: 3 [48000/60000 (80%)]\tLoss: 0.288683\n",
      "Train Epoch: 3 [48640/60000 (81%)]\tLoss: 0.142821\n",
      "Train Epoch: 3 [49280/60000 (82%)]\tLoss: 0.408141\n",
      "Train Epoch: 3 [49920/60000 (83%)]\tLoss: 0.287572\n",
      "Train Epoch: 3 [50560/60000 (84%)]\tLoss: 0.391583\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.311254\n",
      "Train Epoch: 3 [51840/60000 (86%)]\tLoss: 0.501940\n",
      "Train Epoch: 3 [52480/60000 (87%)]\tLoss: 0.421191\n",
      "Train Epoch: 3 [53120/60000 (88%)]\tLoss: 0.343120\n",
      "Train Epoch: 3 [53760/60000 (90%)]\tLoss: 0.314155\n",
      "Train Epoch: 3 [54400/60000 (91%)]\tLoss: 0.333909\n",
      "Train Epoch: 3 [55040/60000 (92%)]\tLoss: 0.149923\n",
      "Train Epoch: 3 [55680/60000 (93%)]\tLoss: 0.290528\n",
      "Train Epoch: 3 [56320/60000 (94%)]\tLoss: 0.238668\n",
      "Train Epoch: 3 [56960/60000 (95%)]\tLoss: 0.426421\n",
      "Train Epoch: 3 [57600/60000 (96%)]\tLoss: 0.523035\n",
      "Train Epoch: 3 [58240/60000 (97%)]\tLoss: 0.276550\n",
      "Train Epoch: 3 [58880/60000 (98%)]\tLoss: 0.411881\n",
      "Train Epoch: 3 [59520/60000 (99%)]\tLoss: 0.274784\n",
      "\n",
      "Test set: Average loss: 0.0048, Accuracy: 9110/10000 (91%)\n",
      "\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.282786\n",
      "Train Epoch: 4 [640/60000 (1%)]\tLoss: 0.289477\n",
      "Train Epoch: 4 [1280/60000 (2%)]\tLoss: 0.142862\n",
      "Train Epoch: 4 [1920/60000 (3%)]\tLoss: 0.338082\n",
      "Train Epoch: 4 [2560/60000 (4%)]\tLoss: 0.293871\n",
      "Train Epoch: 4 [3200/60000 (5%)]\tLoss: 0.391218\n",
      "Train Epoch: 4 [3840/60000 (6%)]\tLoss: 0.272178\n",
      "Train Epoch: 4 [4480/60000 (7%)]\tLoss: 0.183640\n",
      "Train Epoch: 4 [5120/60000 (9%)]\tLoss: 0.455893\n",
      "Train Epoch: 4 [5760/60000 (10%)]\tLoss: 0.248219\n",
      "Train Epoch: 4 [6400/60000 (11%)]\tLoss: 0.498022\n",
      "Train Epoch: 4 [7040/60000 (12%)]\tLoss: 0.332708\n",
      "Train Epoch: 4 [7680/60000 (13%)]\tLoss: 0.228929\n",
      "Train Epoch: 4 [8320/60000 (14%)]\tLoss: 0.339019\n",
      "Train Epoch: 4 [8960/60000 (15%)]\tLoss: 0.360622\n",
      "Train Epoch: 4 [9600/60000 (16%)]\tLoss: 0.220626\n",
      "Train Epoch: 4 [10240/60000 (17%)]\tLoss: 0.331954\n",
      "Train Epoch: 4 [10880/60000 (18%)]\tLoss: 0.411483\n",
      "Train Epoch: 4 [11520/60000 (19%)]\tLoss: 0.348594\n",
      "Train Epoch: 4 [12160/60000 (20%)]\tLoss: 0.221672\n",
      "Train Epoch: 4 [12800/60000 (21%)]\tLoss: 0.337300\n",
      "Train Epoch: 4 [13440/60000 (22%)]\tLoss: 0.176444\n",
      "Train Epoch: 4 [14080/60000 (23%)]\tLoss: 0.151403\n",
      "Train Epoch: 4 [14720/60000 (25%)]\tLoss: 0.354977\n",
      "Train Epoch: 4 [15360/60000 (26%)]\tLoss: 0.281438\n",
      "Train Epoch: 4 [16000/60000 (27%)]\tLoss: 0.295374\n",
      "Train Epoch: 4 [16640/60000 (28%)]\tLoss: 0.336767\n",
      "Train Epoch: 4 [17280/60000 (29%)]\tLoss: 0.413460\n",
      "Train Epoch: 4 [17920/60000 (30%)]\tLoss: 0.438847\n",
      "Train Epoch: 4 [18560/60000 (31%)]\tLoss: 0.213362\n",
      "Train Epoch: 4 [19200/60000 (32%)]\tLoss: 0.279066\n",
      "Train Epoch: 4 [19840/60000 (33%)]\tLoss: 0.441363\n",
      "Train Epoch: 4 [20480/60000 (34%)]\tLoss: 0.238744\n",
      "Train Epoch: 4 [21120/60000 (35%)]\tLoss: 0.228973\n",
      "Train Epoch: 4 [21760/60000 (36%)]\tLoss: 0.290597\n",
      "Train Epoch: 4 [22400/60000 (37%)]\tLoss: 0.373481\n",
      "Train Epoch: 4 [23040/60000 (38%)]\tLoss: 0.167040\n",
      "Train Epoch: 4 [23680/60000 (39%)]\tLoss: 0.162432\n",
      "Train Epoch: 4 [24320/60000 (41%)]\tLoss: 0.280640\n",
      "Train Epoch: 4 [24960/60000 (42%)]\tLoss: 0.139414\n",
      "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 0.513735\n",
      "Train Epoch: 4 [26240/60000 (44%)]\tLoss: 0.273292\n",
      "Train Epoch: 4 [26880/60000 (45%)]\tLoss: 0.282972\n",
      "Train Epoch: 4 [27520/60000 (46%)]\tLoss: 0.486975\n",
      "Train Epoch: 4 [28160/60000 (47%)]\tLoss: 0.202315\n",
      "Train Epoch: 4 [28800/60000 (48%)]\tLoss: 0.192977\n",
      "Train Epoch: 4 [29440/60000 (49%)]\tLoss: 0.583836\n",
      "Train Epoch: 4 [30080/60000 (50%)]\tLoss: 0.314058\n",
      "Train Epoch: 4 [30720/60000 (51%)]\tLoss: 0.175543\n",
      "Train Epoch: 4 [31360/60000 (52%)]\tLoss: 0.211376\n",
      "Train Epoch: 4 [32000/60000 (53%)]\tLoss: 0.243956\n",
      "Train Epoch: 4 [32640/60000 (54%)]\tLoss: 0.263106\n",
      "Train Epoch: 4 [33280/60000 (55%)]\tLoss: 0.170336\n",
      "Train Epoch: 4 [33920/60000 (57%)]\tLoss: 0.338396\n",
      "Train Epoch: 4 [34560/60000 (58%)]\tLoss: 0.213445\n",
      "Train Epoch: 4 [35200/60000 (59%)]\tLoss: 0.189586\n",
      "Train Epoch: 4 [35840/60000 (60%)]\tLoss: 0.340461\n",
      "Train Epoch: 4 [36480/60000 (61%)]\tLoss: 0.403736\n",
      "Train Epoch: 4 [37120/60000 (62%)]\tLoss: 0.305529\n",
      "Train Epoch: 4 [37760/60000 (63%)]\tLoss: 0.221448\n",
      "Train Epoch: 4 [38400/60000 (64%)]\tLoss: 0.223500\n",
      "Train Epoch: 4 [39040/60000 (65%)]\tLoss: 0.181839\n",
      "Train Epoch: 4 [39680/60000 (66%)]\tLoss: 0.305740\n",
      "Train Epoch: 4 [40320/60000 (67%)]\tLoss: 0.252949\n",
      "Train Epoch: 4 [40960/60000 (68%)]\tLoss: 0.322059\n",
      "Train Epoch: 4 [41600/60000 (69%)]\tLoss: 0.147361\n",
      "Train Epoch: 4 [42240/60000 (70%)]\tLoss: 0.160079\n",
      "Train Epoch: 4 [42880/60000 (71%)]\tLoss: 0.464336\n",
      "Train Epoch: 4 [43520/60000 (72%)]\tLoss: 0.109392\n",
      "Train Epoch: 4 [44160/60000 (74%)]\tLoss: 0.466000\n",
      "Train Epoch: 4 [44800/60000 (75%)]\tLoss: 0.215164\n",
      "Train Epoch: 4 [45440/60000 (76%)]\tLoss: 0.323763\n",
      "Train Epoch: 4 [46080/60000 (77%)]\tLoss: 0.277632\n",
      "Train Epoch: 4 [46720/60000 (78%)]\tLoss: 0.294965\n",
      "Train Epoch: 4 [47360/60000 (79%)]\tLoss: 0.314261\n",
      "Train Epoch: 4 [48000/60000 (80%)]\tLoss: 0.251669\n",
      "Train Epoch: 4 [48640/60000 (81%)]\tLoss: 0.314504\n",
      "Train Epoch: 4 [49280/60000 (82%)]\tLoss: 0.338408\n",
      "Train Epoch: 4 [49920/60000 (83%)]\tLoss: 0.172430\n",
      "Train Epoch: 4 [50560/60000 (84%)]\tLoss: 0.137410\n",
      "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 0.360655\n",
      "Train Epoch: 4 [51840/60000 (86%)]\tLoss: 0.362448\n",
      "Train Epoch: 4 [52480/60000 (87%)]\tLoss: 0.213901\n",
      "Train Epoch: 4 [53120/60000 (88%)]\tLoss: 0.239047\n",
      "Train Epoch: 4 [53760/60000 (90%)]\tLoss: 0.148736\n",
      "Train Epoch: 4 [54400/60000 (91%)]\tLoss: 0.267335\n",
      "Train Epoch: 4 [55040/60000 (92%)]\tLoss: 0.138811\n",
      "Train Epoch: 4 [55680/60000 (93%)]\tLoss: 0.523666\n",
      "Train Epoch: 4 [56320/60000 (94%)]\tLoss: 0.270295\n",
      "Train Epoch: 4 [56960/60000 (95%)]\tLoss: 0.197045\n",
      "Train Epoch: 4 [57600/60000 (96%)]\tLoss: 0.265193\n",
      "Train Epoch: 4 [58240/60000 (97%)]\tLoss: 0.277274\n",
      "Train Epoch: 4 [58880/60000 (98%)]\tLoss: 0.227195\n",
      "Train Epoch: 4 [59520/60000 (99%)]\tLoss: 0.416683\n",
      "\n",
      "Test set: Average loss: 0.0035, Accuracy: 9362/10000 (93%)\n",
      "\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.360808\n",
      "Train Epoch: 5 [640/60000 (1%)]\tLoss: 0.369700\n",
      "Train Epoch: 5 [1280/60000 (2%)]\tLoss: 0.217883\n",
      "Train Epoch: 5 [1920/60000 (3%)]\tLoss: 0.229682\n",
      "Train Epoch: 5 [2560/60000 (4%)]\tLoss: 0.203531\n",
      "Train Epoch: 5 [3200/60000 (5%)]\tLoss: 0.332485\n",
      "Train Epoch: 5 [3840/60000 (6%)]\tLoss: 0.319552\n",
      "Train Epoch: 5 [4480/60000 (7%)]\tLoss: 0.213184\n",
      "Train Epoch: 5 [5120/60000 (9%)]\tLoss: 0.115722\n",
      "Train Epoch: 5 [5760/60000 (10%)]\tLoss: 0.232417\n",
      "Train Epoch: 5 [6400/60000 (11%)]\tLoss: 0.391217\n",
      "Train Epoch: 5 [7040/60000 (12%)]\tLoss: 0.464308\n",
      "Train Epoch: 5 [7680/60000 (13%)]\tLoss: 0.053811\n",
      "Train Epoch: 5 [8320/60000 (14%)]\tLoss: 0.091422\n",
      "Train Epoch: 5 [8960/60000 (15%)]\tLoss: 0.137059\n",
      "Train Epoch: 5 [9600/60000 (16%)]\tLoss: 0.185501\n",
      "Train Epoch: 5 [10240/60000 (17%)]\tLoss: 0.241229\n",
      "Train Epoch: 5 [10880/60000 (18%)]\tLoss: 0.144783\n",
      "Train Epoch: 5 [11520/60000 (19%)]\tLoss: 0.287735\n",
      "Train Epoch: 5 [12160/60000 (20%)]\tLoss: 0.046898\n",
      "Train Epoch: 5 [12800/60000 (21%)]\tLoss: 0.267244\n",
      "Train Epoch: 5 [13440/60000 (22%)]\tLoss: 0.175136\n",
      "Train Epoch: 5 [14080/60000 (23%)]\tLoss: 0.225472\n",
      "Train Epoch: 5 [14720/60000 (25%)]\tLoss: 0.212508\n",
      "Train Epoch: 5 [15360/60000 (26%)]\tLoss: 0.299038\n",
      "Train Epoch: 5 [16000/60000 (27%)]\tLoss: 0.154294\n",
      "Train Epoch: 5 [16640/60000 (28%)]\tLoss: 0.129645\n",
      "Train Epoch: 5 [17280/60000 (29%)]\tLoss: 0.180034\n",
      "Train Epoch: 5 [17920/60000 (30%)]\tLoss: 0.189329\n",
      "Train Epoch: 5 [18560/60000 (31%)]\tLoss: 0.240242\n",
      "Train Epoch: 5 [19200/60000 (32%)]\tLoss: 0.277851\n",
      "Train Epoch: 5 [19840/60000 (33%)]\tLoss: 0.098569\n",
      "Train Epoch: 5 [20480/60000 (34%)]\tLoss: 0.422941\n",
      "Train Epoch: 5 [21120/60000 (35%)]\tLoss: 0.189710\n",
      "Train Epoch: 5 [21760/60000 (36%)]\tLoss: 0.295227\n",
      "Train Epoch: 5 [22400/60000 (37%)]\tLoss: 0.376452\n",
      "Train Epoch: 5 [23040/60000 (38%)]\tLoss: 0.196802\n",
      "Train Epoch: 5 [23680/60000 (39%)]\tLoss: 0.261006\n",
      "Train Epoch: 5 [24320/60000 (41%)]\tLoss: 0.367042\n",
      "Train Epoch: 5 [24960/60000 (42%)]\tLoss: 0.233406\n",
      "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 0.352884\n",
      "Train Epoch: 5 [26240/60000 (44%)]\tLoss: 0.458294\n",
      "Train Epoch: 5 [26880/60000 (45%)]\tLoss: 0.253421\n",
      "Train Epoch: 5 [27520/60000 (46%)]\tLoss: 0.121562\n",
      "Train Epoch: 5 [28160/60000 (47%)]\tLoss: 0.354898\n",
      "Train Epoch: 5 [28800/60000 (48%)]\tLoss: 0.105810\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 5 [29440/60000 (49%)]\tLoss: 0.128302\n",
      "Train Epoch: 5 [30080/60000 (50%)]\tLoss: 0.112098\n",
      "Train Epoch: 5 [30720/60000 (51%)]\tLoss: 0.175625\n",
      "Train Epoch: 5 [31360/60000 (52%)]\tLoss: 0.167498\n",
      "Train Epoch: 5 [32000/60000 (53%)]\tLoss: 0.279384\n",
      "Train Epoch: 5 [32640/60000 (54%)]\tLoss: 0.162030\n",
      "Train Epoch: 5 [33280/60000 (55%)]\tLoss: 0.322968\n",
      "Train Epoch: 5 [33920/60000 (57%)]\tLoss: 0.328161\n",
      "Train Epoch: 5 [34560/60000 (58%)]\tLoss: 0.161203\n",
      "Train Epoch: 5 [35200/60000 (59%)]\tLoss: 0.373350\n",
      "Train Epoch: 5 [35840/60000 (60%)]\tLoss: 0.329837\n",
      "Train Epoch: 5 [36480/60000 (61%)]\tLoss: 0.177063\n",
      "Train Epoch: 5 [37120/60000 (62%)]\tLoss: 0.191305\n",
      "Train Epoch: 5 [37760/60000 (63%)]\tLoss: 0.139932\n",
      "Train Epoch: 5 [38400/60000 (64%)]\tLoss: 0.141715\n",
      "Train Epoch: 5 [39040/60000 (65%)]\tLoss: 0.238406\n",
      "Train Epoch: 5 [39680/60000 (66%)]\tLoss: 0.070460\n",
      "Train Epoch: 5 [40320/60000 (67%)]\tLoss: 0.174106\n",
      "Train Epoch: 5 [40960/60000 (68%)]\tLoss: 0.395708\n",
      "Train Epoch: 5 [41600/60000 (69%)]\tLoss: 0.131053\n",
      "Train Epoch: 5 [42240/60000 (70%)]\tLoss: 0.179433\n",
      "Train Epoch: 5 [42880/60000 (71%)]\tLoss: 0.221904\n",
      "Train Epoch: 5 [43520/60000 (72%)]\tLoss: 0.313884\n",
      "Train Epoch: 5 [44160/60000 (74%)]\tLoss: 0.092215\n",
      "Train Epoch: 5 [44800/60000 (75%)]\tLoss: 0.085802\n",
      "Train Epoch: 5 [45440/60000 (76%)]\tLoss: 0.060092\n",
      "Train Epoch: 5 [46080/60000 (77%)]\tLoss: 0.292488\n",
      "Train Epoch: 5 [46720/60000 (78%)]\tLoss: 0.154695\n",
      "Train Epoch: 5 [47360/60000 (79%)]\tLoss: 0.068577\n",
      "Train Epoch: 5 [48000/60000 (80%)]\tLoss: 0.346677\n",
      "Train Epoch: 5 [48640/60000 (81%)]\tLoss: 0.035294\n",
      "Train Epoch: 5 [49280/60000 (82%)]\tLoss: 0.223755\n",
      "Train Epoch: 5 [49920/60000 (83%)]\tLoss: 0.130114\n",
      "Train Epoch: 5 [50560/60000 (84%)]\tLoss: 0.112758\n",
      "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 0.147092\n",
      "Train Epoch: 5 [51840/60000 (86%)]\tLoss: 0.164960\n",
      "Train Epoch: 5 [52480/60000 (87%)]\tLoss: 0.173469\n",
      "Train Epoch: 5 [53120/60000 (88%)]\tLoss: 0.174930\n",
      "Train Epoch: 5 [53760/60000 (90%)]\tLoss: 0.272455\n",
      "Train Epoch: 5 [54400/60000 (91%)]\tLoss: 0.290726\n",
      "Train Epoch: 5 [55040/60000 (92%)]\tLoss: 0.203008\n",
      "Train Epoch: 5 [55680/60000 (93%)]\tLoss: 0.092579\n",
      "Train Epoch: 5 [56320/60000 (94%)]\tLoss: 0.315970\n",
      "Train Epoch: 5 [56960/60000 (95%)]\tLoss: 0.121381\n",
      "Train Epoch: 5 [57600/60000 (96%)]\tLoss: 0.156930\n",
      "Train Epoch: 5 [58240/60000 (97%)]\tLoss: 0.206395\n",
      "Train Epoch: 5 [58880/60000 (98%)]\tLoss: 0.180272\n",
      "Train Epoch: 5 [59520/60000 (99%)]\tLoss: 0.176613\n",
      "\n",
      "Test set: Average loss: 0.0030, Accuracy: 9424/10000 (94%)\n",
      "\n",
      "Train Epoch: 6 [0/60000 (0%)]\tLoss: 0.168077\n",
      "Train Epoch: 6 [640/60000 (1%)]\tLoss: 0.328073\n",
      "Train Epoch: 6 [1280/60000 (2%)]\tLoss: 0.087497\n",
      "Train Epoch: 6 [1920/60000 (3%)]\tLoss: 0.126770\n",
      "Train Epoch: 6 [2560/60000 (4%)]\tLoss: 0.256015\n",
      "Train Epoch: 6 [3200/60000 (5%)]\tLoss: 0.111959\n",
      "Train Epoch: 6 [3840/60000 (6%)]\tLoss: 0.139017\n",
      "Train Epoch: 6 [4480/60000 (7%)]\tLoss: 0.266771\n",
      "Train Epoch: 6 [5120/60000 (9%)]\tLoss: 0.051642\n",
      "Train Epoch: 6 [5760/60000 (10%)]\tLoss: 0.238828\n",
      "Train Epoch: 6 [6400/60000 (11%)]\tLoss: 0.113531\n",
      "Train Epoch: 6 [7040/60000 (12%)]\tLoss: 0.213458\n",
      "Train Epoch: 6 [7680/60000 (13%)]\tLoss: 0.105465\n",
      "Train Epoch: 6 [8320/60000 (14%)]\tLoss: 0.135096\n",
      "Train Epoch: 6 [8960/60000 (15%)]\tLoss: 0.287567\n",
      "Train Epoch: 6 [9600/60000 (16%)]\tLoss: 0.200848\n",
      "Train Epoch: 6 [10240/60000 (17%)]\tLoss: 0.103616\n",
      "Train Epoch: 6 [10880/60000 (18%)]\tLoss: 0.256211\n",
      "Train Epoch: 6 [11520/60000 (19%)]\tLoss: 0.156749\n",
      "Train Epoch: 6 [12160/60000 (20%)]\tLoss: 0.160294\n",
      "Train Epoch: 6 [12800/60000 (21%)]\tLoss: 0.169755\n",
      "Train Epoch: 6 [13440/60000 (22%)]\tLoss: 0.199262\n",
      "Train Epoch: 6 [14080/60000 (23%)]\tLoss: 0.155845\n",
      "Train Epoch: 6 [14720/60000 (25%)]\tLoss: 0.198595\n",
      "Train Epoch: 6 [15360/60000 (26%)]\tLoss: 0.120754\n",
      "Train Epoch: 6 [16000/60000 (27%)]\tLoss: 0.103237\n",
      "Train Epoch: 6 [16640/60000 (28%)]\tLoss: 0.186685\n",
      "Train Epoch: 6 [17280/60000 (29%)]\tLoss: 0.267880\n",
      "Train Epoch: 6 [17920/60000 (30%)]\tLoss: 0.144914\n",
      "Train Epoch: 6 [18560/60000 (31%)]\tLoss: 0.147712\n",
      "Train Epoch: 6 [19200/60000 (32%)]\tLoss: 0.158992\n",
      "Train Epoch: 6 [19840/60000 (33%)]\tLoss: 0.154123\n",
      "Train Epoch: 6 [20480/60000 (34%)]\tLoss: 0.115288\n",
      "Train Epoch: 6 [21120/60000 (35%)]\tLoss: 0.131510\n",
      "Train Epoch: 6 [21760/60000 (36%)]\tLoss: 0.229929\n",
      "Train Epoch: 6 [22400/60000 (37%)]\tLoss: 0.133731\n",
      "Train Epoch: 6 [23040/60000 (38%)]\tLoss: 0.129227\n",
      "Train Epoch: 6 [23680/60000 (39%)]\tLoss: 0.204614\n",
      "Train Epoch: 6 [24320/60000 (41%)]\tLoss: 0.184474\n",
      "Train Epoch: 6 [24960/60000 (42%)]\tLoss: 0.321393\n",
      "Train Epoch: 6 [25600/60000 (43%)]\tLoss: 0.072440\n",
      "Train Epoch: 6 [26240/60000 (44%)]\tLoss: 0.113100\n",
      "Train Epoch: 6 [26880/60000 (45%)]\tLoss: 0.085865\n",
      "Train Epoch: 6 [27520/60000 (46%)]\tLoss: 0.108050\n",
      "Train Epoch: 6 [28160/60000 (47%)]\tLoss: 0.053191\n",
      "Train Epoch: 6 [28800/60000 (48%)]\tLoss: 0.128308\n",
      "Train Epoch: 6 [29440/60000 (49%)]\tLoss: 0.214448\n",
      "Train Epoch: 6 [30080/60000 (50%)]\tLoss: 0.284490\n",
      "Train Epoch: 6 [30720/60000 (51%)]\tLoss: 0.096583\n",
      "Train Epoch: 6 [31360/60000 (52%)]\tLoss: 0.095466\n",
      "Train Epoch: 6 [32000/60000 (53%)]\tLoss: 0.243790\n",
      "Train Epoch: 6 [32640/60000 (54%)]\tLoss: 0.224886\n",
      "Train Epoch: 6 [33280/60000 (55%)]\tLoss: 0.117274\n",
      "Train Epoch: 6 [33920/60000 (57%)]\tLoss: 0.156581\n",
      "Train Epoch: 6 [34560/60000 (58%)]\tLoss: 0.150858\n",
      "Train Epoch: 6 [35200/60000 (59%)]\tLoss: 0.083845\n",
      "Train Epoch: 6 [35840/60000 (60%)]\tLoss: 0.059690\n",
      "Train Epoch: 6 [36480/60000 (61%)]\tLoss: 0.125617\n",
      "Train Epoch: 6 [37120/60000 (62%)]\tLoss: 0.152609\n",
      "Train Epoch: 6 [37760/60000 (63%)]\tLoss: 0.132903\n",
      "Train Epoch: 6 [38400/60000 (64%)]\tLoss: 0.157891\n",
      "Train Epoch: 6 [39040/60000 (65%)]\tLoss: 0.359563\n",
      "Train Epoch: 6 [39680/60000 (66%)]\tLoss: 0.099740\n",
      "Train Epoch: 6 [40320/60000 (67%)]\tLoss: 0.135612\n",
      "Train Epoch: 6 [40960/60000 (68%)]\tLoss: 0.150505\n",
      "Train Epoch: 6 [41600/60000 (69%)]\tLoss: 0.049368\n",
      "Train Epoch: 6 [42240/60000 (70%)]\tLoss: 0.053331\n",
      "Train Epoch: 6 [42880/60000 (71%)]\tLoss: 0.162272\n",
      "Train Epoch: 6 [43520/60000 (72%)]\tLoss: 0.131214\n",
      "Train Epoch: 6 [44160/60000 (74%)]\tLoss: 0.105007\n",
      "Train Epoch: 6 [44800/60000 (75%)]\tLoss: 0.275490\n",
      "Train Epoch: 6 [45440/60000 (76%)]\tLoss: 0.119026\n",
      "Train Epoch: 6 [46080/60000 (77%)]\tLoss: 0.201803\n",
      "Train Epoch: 6 [46720/60000 (78%)]\tLoss: 0.142809\n",
      "Train Epoch: 6 [47360/60000 (79%)]\tLoss: 0.162959\n",
      "Train Epoch: 6 [48000/60000 (80%)]\tLoss: 0.096226\n",
      "Train Epoch: 6 [48640/60000 (81%)]\tLoss: 0.205613\n",
      "Train Epoch: 6 [49280/60000 (82%)]\tLoss: 0.323878\n",
      "Train Epoch: 6 [49920/60000 (83%)]\tLoss: 0.129257\n",
      "Train Epoch: 6 [50560/60000 (84%)]\tLoss: 0.140741\n",
      "Train Epoch: 6 [51200/60000 (85%)]\tLoss: 0.119399\n",
      "Train Epoch: 6 [51840/60000 (86%)]\tLoss: 0.304955\n",
      "Train Epoch: 6 [52480/60000 (87%)]\tLoss: 0.143883\n",
      "Train Epoch: 6 [53120/60000 (88%)]\tLoss: 0.081649\n",
      "Train Epoch: 6 [53760/60000 (90%)]\tLoss: 0.374451\n",
      "Train Epoch: 6 [54400/60000 (91%)]\tLoss: 0.105486\n",
      "Train Epoch: 6 [55040/60000 (92%)]\tLoss: 0.176809\n",
      "Train Epoch: 6 [55680/60000 (93%)]\tLoss: 0.299361\n",
      "Train Epoch: 6 [56320/60000 (94%)]\tLoss: 0.078326\n",
      "Train Epoch: 6 [56960/60000 (95%)]\tLoss: 0.136160\n",
      "Train Epoch: 6 [57600/60000 (96%)]\tLoss: 0.213388\n",
      "Train Epoch: 6 [58240/60000 (97%)]\tLoss: 0.075368\n",
      "Train Epoch: 6 [58880/60000 (98%)]\tLoss: 0.071270\n",
      "Train Epoch: 6 [59520/60000 (99%)]\tLoss: 0.112452\n",
      "\n",
      "Test set: Average loss: 0.0023, Accuracy: 9577/10000 (95%)\n",
      "\n",
      "Train Epoch: 7 [0/60000 (0%)]\tLoss: 0.119897\n",
      "Train Epoch: 7 [640/60000 (1%)]\tLoss: 0.243516\n",
      "Train Epoch: 7 [1280/60000 (2%)]\tLoss: 0.061277\n",
      "Train Epoch: 7 [1920/60000 (3%)]\tLoss: 0.109300\n",
      "Train Epoch: 7 [2560/60000 (4%)]\tLoss: 0.077637\n",
      "Train Epoch: 7 [3200/60000 (5%)]\tLoss: 0.080124\n",
      "Train Epoch: 7 [3840/60000 (6%)]\tLoss: 0.056445\n",
      "Train Epoch: 7 [4480/60000 (7%)]\tLoss: 0.268141\n",
      "Train Epoch: 7 [5120/60000 (9%)]\tLoss: 0.227599\n",
      "Train Epoch: 7 [5760/60000 (10%)]\tLoss: 0.221190\n",
      "Train Epoch: 7 [6400/60000 (11%)]\tLoss: 0.182657\n",
      "Train Epoch: 7 [7040/60000 (12%)]\tLoss: 0.116882\n",
      "Train Epoch: 7 [7680/60000 (13%)]\tLoss: 0.055066\n",
      "Train Epoch: 7 [8320/60000 (14%)]\tLoss: 0.172173\n",
      "Train Epoch: 7 [8960/60000 (15%)]\tLoss: 0.138729\n",
      "Train Epoch: 7 [9600/60000 (16%)]\tLoss: 0.165927\n",
      "Train Epoch: 7 [10240/60000 (17%)]\tLoss: 0.139068\n",
      "Train Epoch: 7 [10880/60000 (18%)]\tLoss: 0.174699\n",
      "Train Epoch: 7 [11520/60000 (19%)]\tLoss: 0.099463\n",
      "Train Epoch: 7 [12160/60000 (20%)]\tLoss: 0.232003\n",
      "Train Epoch: 7 [12800/60000 (21%)]\tLoss: 0.177766\n",
      "Train Epoch: 7 [13440/60000 (22%)]\tLoss: 0.053840\n",
      "Train Epoch: 7 [14080/60000 (23%)]\tLoss: 0.155319\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 7 [14720/60000 (25%)]\tLoss: 0.148235\n",
      "Train Epoch: 7 [15360/60000 (26%)]\tLoss: 0.211220\n",
      "Train Epoch: 7 [16000/60000 (27%)]\tLoss: 0.083867\n",
      "Train Epoch: 7 [16640/60000 (28%)]\tLoss: 0.122764\n",
      "Train Epoch: 7 [17280/60000 (29%)]\tLoss: 0.303161\n",
      "Train Epoch: 7 [17920/60000 (30%)]\tLoss: 0.068525\n",
      "Train Epoch: 7 [18560/60000 (31%)]\tLoss: 0.058306\n",
      "Train Epoch: 7 [19200/60000 (32%)]\tLoss: 0.189825\n",
      "Train Epoch: 7 [19840/60000 (33%)]\tLoss: 0.119065\n",
      "Train Epoch: 7 [20480/60000 (34%)]\tLoss: 0.036156\n",
      "Train Epoch: 7 [21120/60000 (35%)]\tLoss: 0.080072\n",
      "Train Epoch: 7 [21760/60000 (36%)]\tLoss: 0.247375\n",
      "Train Epoch: 7 [22400/60000 (37%)]\tLoss: 0.177764\n",
      "Train Epoch: 7 [23040/60000 (38%)]\tLoss: 0.130488\n",
      "Train Epoch: 7 [23680/60000 (39%)]\tLoss: 0.089504\n",
      "Train Epoch: 7 [24320/60000 (41%)]\tLoss: 0.309280\n",
      "Train Epoch: 7 [24960/60000 (42%)]\tLoss: 0.190800\n",
      "Train Epoch: 7 [25600/60000 (43%)]\tLoss: 0.194975\n",
      "Train Epoch: 7 [26240/60000 (44%)]\tLoss: 0.101764\n",
      "Train Epoch: 7 [26880/60000 (45%)]\tLoss: 0.205628\n",
      "Train Epoch: 7 [27520/60000 (46%)]\tLoss: 0.198913\n",
      "Train Epoch: 7 [28160/60000 (47%)]\tLoss: 0.161951\n",
      "Train Epoch: 7 [28800/60000 (48%)]\tLoss: 0.418600\n",
      "Train Epoch: 7 [29440/60000 (49%)]\tLoss: 0.048091\n",
      "Train Epoch: 7 [30080/60000 (50%)]\tLoss: 0.060851\n",
      "Train Epoch: 7 [30720/60000 (51%)]\tLoss: 0.037745\n",
      "Train Epoch: 7 [31360/60000 (52%)]\tLoss: 0.034792\n",
      "Train Epoch: 7 [32000/60000 (53%)]\tLoss: 0.094320\n",
      "Train Epoch: 7 [32640/60000 (54%)]\tLoss: 0.211765\n",
      "Train Epoch: 7 [33280/60000 (55%)]\tLoss: 0.254535\n",
      "Train Epoch: 7 [33920/60000 (57%)]\tLoss: 0.083615\n",
      "Train Epoch: 7 [34560/60000 (58%)]\tLoss: 0.227957\n",
      "Train Epoch: 7 [35200/60000 (59%)]\tLoss: 0.096063\n",
      "Train Epoch: 7 [35840/60000 (60%)]\tLoss: 0.168323\n",
      "Train Epoch: 7 [36480/60000 (61%)]\tLoss: 0.174719\n",
      "Train Epoch: 7 [37120/60000 (62%)]\tLoss: 0.091972\n",
      "Train Epoch: 7 [37760/60000 (63%)]\tLoss: 0.104866\n",
      "Train Epoch: 7 [38400/60000 (64%)]\tLoss: 0.111251\n",
      "Train Epoch: 7 [39040/60000 (65%)]\tLoss: 0.058099\n",
      "Train Epoch: 7 [39680/60000 (66%)]\tLoss: 0.152239\n",
      "Train Epoch: 7 [40320/60000 (67%)]\tLoss: 0.093595\n",
      "Train Epoch: 7 [40960/60000 (68%)]\tLoss: 0.052223\n",
      "Train Epoch: 7 [41600/60000 (69%)]\tLoss: 0.096222\n",
      "Train Epoch: 7 [42240/60000 (70%)]\tLoss: 0.093554\n",
      "Train Epoch: 7 [42880/60000 (71%)]\tLoss: 0.037717\n",
      "Train Epoch: 7 [43520/60000 (72%)]\tLoss: 0.086526\n",
      "Train Epoch: 7 [44160/60000 (74%)]\tLoss: 0.152295\n",
      "Train Epoch: 7 [44800/60000 (75%)]\tLoss: 0.037722\n",
      "Train Epoch: 7 [45440/60000 (76%)]\tLoss: 0.040068\n",
      "Train Epoch: 7 [46080/60000 (77%)]\tLoss: 0.193245\n",
      "Train Epoch: 7 [46720/60000 (78%)]\tLoss: 0.043504\n",
      "Train Epoch: 7 [47360/60000 (79%)]\tLoss: 0.061344\n",
      "Train Epoch: 7 [48000/60000 (80%)]\tLoss: 0.055311\n",
      "Train Epoch: 7 [48640/60000 (81%)]\tLoss: 0.325291\n",
      "Train Epoch: 7 [49280/60000 (82%)]\tLoss: 0.224439\n",
      "Train Epoch: 7 [49920/60000 (83%)]\tLoss: 0.043813\n",
      "Train Epoch: 7 [50560/60000 (84%)]\tLoss: 0.198407\n",
      "Train Epoch: 7 [51200/60000 (85%)]\tLoss: 0.124929\n",
      "Train Epoch: 7 [51840/60000 (86%)]\tLoss: 0.119698\n",
      "Train Epoch: 7 [52480/60000 (87%)]\tLoss: 0.132226\n",
      "Train Epoch: 7 [53120/60000 (88%)]\tLoss: 0.081878\n",
      "Train Epoch: 7 [53760/60000 (90%)]\tLoss: 0.063080\n",
      "Train Epoch: 7 [54400/60000 (91%)]\tLoss: 0.173053\n",
      "Train Epoch: 7 [55040/60000 (92%)]\tLoss: 0.054817\n",
      "Train Epoch: 7 [55680/60000 (93%)]\tLoss: 0.171784\n",
      "Train Epoch: 7 [56320/60000 (94%)]\tLoss: 0.117012\n",
      "Train Epoch: 7 [56960/60000 (95%)]\tLoss: 0.136308\n",
      "Train Epoch: 7 [57600/60000 (96%)]\tLoss: 0.180419\n",
      "Train Epoch: 7 [58240/60000 (97%)]\tLoss: 0.083362\n",
      "Train Epoch: 7 [58880/60000 (98%)]\tLoss: 0.102999\n",
      "Train Epoch: 7 [59520/60000 (99%)]\tLoss: 0.103052\n",
      "\n",
      "Test set: Average loss: 0.0020, Accuracy: 9607/10000 (96%)\n",
      "\n",
      "Train Epoch: 8 [0/60000 (0%)]\tLoss: 0.111526\n",
      "Train Epoch: 8 [640/60000 (1%)]\tLoss: 0.074498\n",
      "Train Epoch: 8 [1280/60000 (2%)]\tLoss: 0.106909\n",
      "Train Epoch: 8 [1920/60000 (3%)]\tLoss: 0.089368\n",
      "Train Epoch: 8 [2560/60000 (4%)]\tLoss: 0.059127\n",
      "Train Epoch: 8 [3200/60000 (5%)]\tLoss: 0.078058\n",
      "Train Epoch: 8 [3840/60000 (6%)]\tLoss: 0.157765\n",
      "Train Epoch: 8 [4480/60000 (7%)]\tLoss: 0.204855\n",
      "Train Epoch: 8 [5120/60000 (9%)]\tLoss: 0.189524\n",
      "Train Epoch: 8 [5760/60000 (10%)]\tLoss: 0.050948\n",
      "Train Epoch: 8 [6400/60000 (11%)]\tLoss: 0.328487\n",
      "Train Epoch: 8 [7040/60000 (12%)]\tLoss: 0.080301\n",
      "Train Epoch: 8 [7680/60000 (13%)]\tLoss: 0.065496\n",
      "Train Epoch: 8 [8320/60000 (14%)]\tLoss: 0.123145\n",
      "Train Epoch: 8 [8960/60000 (15%)]\tLoss: 0.195842\n",
      "Train Epoch: 8 [9600/60000 (16%)]\tLoss: 0.134604\n",
      "Train Epoch: 8 [10240/60000 (17%)]\tLoss: 0.081253\n",
      "Train Epoch: 8 [10880/60000 (18%)]\tLoss: 0.078178\n",
      "Train Epoch: 8 [11520/60000 (19%)]\tLoss: 0.214193\n",
      "Train Epoch: 8 [12160/60000 (20%)]\tLoss: 0.184252\n",
      "Train Epoch: 8 [12800/60000 (21%)]\tLoss: 0.134484\n",
      "Train Epoch: 8 [13440/60000 (22%)]\tLoss: 0.217286\n",
      "Train Epoch: 8 [14080/60000 (23%)]\tLoss: 0.269658\n",
      "Train Epoch: 8 [14720/60000 (25%)]\tLoss: 0.080773\n",
      "Train Epoch: 8 [15360/60000 (26%)]\tLoss: 0.035973\n",
      "Train Epoch: 8 [16000/60000 (27%)]\tLoss: 0.026213\n",
      "Train Epoch: 8 [16640/60000 (28%)]\tLoss: 0.077908\n",
      "Train Epoch: 8 [17280/60000 (29%)]\tLoss: 0.038118\n",
      "Train Epoch: 8 [17920/60000 (30%)]\tLoss: 0.248300\n",
      "Train Epoch: 8 [18560/60000 (31%)]\tLoss: 0.162111\n",
      "Train Epoch: 8 [19200/60000 (32%)]\tLoss: 0.158511\n",
      "Train Epoch: 8 [19840/60000 (33%)]\tLoss: 0.068294\n",
      "Train Epoch: 8 [20480/60000 (34%)]\tLoss: 0.137902\n",
      "Train Epoch: 8 [21120/60000 (35%)]\tLoss: 0.163822\n",
      "Train Epoch: 8 [21760/60000 (36%)]\tLoss: 0.088679\n",
      "Train Epoch: 8 [22400/60000 (37%)]\tLoss: 0.059852\n",
      "Train Epoch: 8 [23040/60000 (38%)]\tLoss: 0.110199\n",
      "Train Epoch: 8 [23680/60000 (39%)]\tLoss: 0.052162\n",
      "Train Epoch: 8 [24320/60000 (41%)]\tLoss: 0.035391\n",
      "Train Epoch: 8 [24960/60000 (42%)]\tLoss: 0.099376\n",
      "Train Epoch: 8 [25600/60000 (43%)]\tLoss: 0.040127\n",
      "Train Epoch: 8 [26240/60000 (44%)]\tLoss: 0.135892\n",
      "Train Epoch: 8 [26880/60000 (45%)]\tLoss: 0.039253\n",
      "Train Epoch: 8 [27520/60000 (46%)]\tLoss: 0.060078\n",
      "Train Epoch: 8 [28160/60000 (47%)]\tLoss: 0.026007\n",
      "Train Epoch: 8 [28800/60000 (48%)]\tLoss: 0.100544\n",
      "Train Epoch: 8 [29440/60000 (49%)]\tLoss: 0.028159\n",
      "Train Epoch: 8 [30080/60000 (50%)]\tLoss: 0.153478\n",
      "Train Epoch: 8 [30720/60000 (51%)]\tLoss: 0.105371\n",
      "Train Epoch: 8 [31360/60000 (52%)]\tLoss: 0.179382\n",
      "Train Epoch: 8 [32000/60000 (53%)]\tLoss: 0.070921\n",
      "Train Epoch: 8 [32640/60000 (54%)]\tLoss: 0.121534\n",
      "Train Epoch: 8 [33280/60000 (55%)]\tLoss: 0.209888\n",
      "Train Epoch: 8 [33920/60000 (57%)]\tLoss: 0.068349\n",
      "Train Epoch: 8 [34560/60000 (58%)]\tLoss: 0.124750\n",
      "Train Epoch: 8 [35200/60000 (59%)]\tLoss: 0.140772\n",
      "Train Epoch: 8 [35840/60000 (60%)]\tLoss: 0.033254\n",
      "Train Epoch: 8 [36480/60000 (61%)]\tLoss: 0.256432\n",
      "Train Epoch: 8 [37120/60000 (62%)]\tLoss: 0.064783\n",
      "Train Epoch: 8 [37760/60000 (63%)]\tLoss: 0.143342\n",
      "Train Epoch: 8 [38400/60000 (64%)]\tLoss: 0.108917\n",
      "Train Epoch: 8 [39040/60000 (65%)]\tLoss: 0.137265\n",
      "Train Epoch: 8 [39680/60000 (66%)]\tLoss: 0.053676\n",
      "Train Epoch: 8 [40320/60000 (67%)]\tLoss: 0.058413\n",
      "Train Epoch: 8 [40960/60000 (68%)]\tLoss: 0.080425\n",
      "Train Epoch: 8 [41600/60000 (69%)]\tLoss: 0.127270\n",
      "Train Epoch: 8 [42240/60000 (70%)]\tLoss: 0.088600\n",
      "Train Epoch: 8 [42880/60000 (71%)]\tLoss: 0.080670\n",
      "Train Epoch: 8 [43520/60000 (72%)]\tLoss: 0.132788\n",
      "Train Epoch: 8 [44160/60000 (74%)]\tLoss: 0.054101\n",
      "Train Epoch: 8 [44800/60000 (75%)]\tLoss: 0.078925\n",
      "Train Epoch: 8 [45440/60000 (76%)]\tLoss: 0.018204\n",
      "Train Epoch: 8 [46080/60000 (77%)]\tLoss: 0.067601\n",
      "Train Epoch: 8 [46720/60000 (78%)]\tLoss: 0.038654\n",
      "Train Epoch: 8 [47360/60000 (79%)]\tLoss: 0.050044\n",
      "Train Epoch: 8 [48000/60000 (80%)]\tLoss: 0.083575\n",
      "Train Epoch: 8 [48640/60000 (81%)]\tLoss: 0.305029\n",
      "Train Epoch: 8 [49280/60000 (82%)]\tLoss: 0.181027\n",
      "Train Epoch: 8 [49920/60000 (83%)]\tLoss: 0.068682\n",
      "Train Epoch: 8 [50560/60000 (84%)]\tLoss: 0.212840\n",
      "Train Epoch: 8 [51200/60000 (85%)]\tLoss: 0.138817\n",
      "Train Epoch: 8 [51840/60000 (86%)]\tLoss: 0.116157\n",
      "Train Epoch: 8 [52480/60000 (87%)]\tLoss: 0.064106\n",
      "Train Epoch: 8 [53120/60000 (88%)]\tLoss: 0.105946\n",
      "Train Epoch: 8 [53760/60000 (90%)]\tLoss: 0.071842\n",
      "Train Epoch: 8 [54400/60000 (91%)]\tLoss: 0.080719\n",
      "Train Epoch: 8 [55040/60000 (92%)]\tLoss: 0.047971\n",
      "Train Epoch: 8 [55680/60000 (93%)]\tLoss: 0.148182\n",
      "Train Epoch: 8 [56320/60000 (94%)]\tLoss: 0.085706\n",
      "Train Epoch: 8 [56960/60000 (95%)]\tLoss: 0.216769\n",
      "Train Epoch: 8 [57600/60000 (96%)]\tLoss: 0.077304\n",
      "Train Epoch: 8 [58240/60000 (97%)]\tLoss: 0.048105\n",
      "Train Epoch: 8 [58880/60000 (98%)]\tLoss: 0.153900\n",
      "Train Epoch: 8 [59520/60000 (99%)]\tLoss: 0.070273\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.0019, Accuracy: 9638/10000 (96%)\n",
      "\n",
      "Train Epoch: 9 [0/60000 (0%)]\tLoss: 0.042091\n",
      "Train Epoch: 9 [640/60000 (1%)]\tLoss: 0.169063\n",
      "Train Epoch: 9 [1280/60000 (2%)]\tLoss: 0.054292\n",
      "Train Epoch: 9 [1920/60000 (3%)]\tLoss: 0.076223\n",
      "Train Epoch: 9 [2560/60000 (4%)]\tLoss: 0.044938\n",
      "Train Epoch: 9 [3200/60000 (5%)]\tLoss: 0.095735\n",
      "Train Epoch: 9 [3840/60000 (6%)]\tLoss: 0.019393\n",
      "Train Epoch: 9 [4480/60000 (7%)]\tLoss: 0.037480\n",
      "Train Epoch: 9 [5120/60000 (9%)]\tLoss: 0.133734\n",
      "Train Epoch: 9 [5760/60000 (10%)]\tLoss: 0.254806\n",
      "Train Epoch: 9 [6400/60000 (11%)]\tLoss: 0.062172\n",
      "Train Epoch: 9 [7040/60000 (12%)]\tLoss: 0.089260\n",
      "Train Epoch: 9 [7680/60000 (13%)]\tLoss: 0.039551\n",
      "Train Epoch: 9 [8320/60000 (14%)]\tLoss: 0.101365\n",
      "Train Epoch: 9 [8960/60000 (15%)]\tLoss: 0.064731\n",
      "Train Epoch: 9 [9600/60000 (16%)]\tLoss: 0.136905\n",
      "Train Epoch: 9 [10240/60000 (17%)]\tLoss: 0.042417\n",
      "Train Epoch: 9 [10880/60000 (18%)]\tLoss: 0.089013\n",
      "Train Epoch: 9 [11520/60000 (19%)]\tLoss: 0.203547\n",
      "Train Epoch: 9 [12160/60000 (20%)]\tLoss: 0.030047\n",
      "Train Epoch: 9 [12800/60000 (21%)]\tLoss: 0.055456\n",
      "Train Epoch: 9 [13440/60000 (22%)]\tLoss: 0.061364\n",
      "Train Epoch: 9 [14080/60000 (23%)]\tLoss: 0.157856\n",
      "Train Epoch: 9 [14720/60000 (25%)]\tLoss: 0.079325\n",
      "Train Epoch: 9 [15360/60000 (26%)]\tLoss: 0.056800\n",
      "Train Epoch: 9 [16000/60000 (27%)]\tLoss: 0.078858\n",
      "Train Epoch: 9 [16640/60000 (28%)]\tLoss: 0.047937\n",
      "Train Epoch: 9 [17280/60000 (29%)]\tLoss: 0.070537\n",
      "Train Epoch: 9 [17920/60000 (30%)]\tLoss: 0.161049\n",
      "Train Epoch: 9 [18560/60000 (31%)]\tLoss: 0.070640\n",
      "Train Epoch: 9 [19200/60000 (32%)]\tLoss: 0.159313\n",
      "Train Epoch: 9 [19840/60000 (33%)]\tLoss: 0.081622\n",
      "Train Epoch: 9 [20480/60000 (34%)]\tLoss: 0.085643\n",
      "Train Epoch: 9 [21120/60000 (35%)]\tLoss: 0.103911\n",
      "Train Epoch: 9 [21760/60000 (36%)]\tLoss: 0.031591\n",
      "Train Epoch: 9 [22400/60000 (37%)]\tLoss: 0.032719\n",
      "Train Epoch: 9 [23040/60000 (38%)]\tLoss: 0.109947\n",
      "Train Epoch: 9 [23680/60000 (39%)]\tLoss: 0.060668\n",
      "Train Epoch: 9 [24320/60000 (41%)]\tLoss: 0.045247\n",
      "Train Epoch: 9 [24960/60000 (42%)]\tLoss: 0.028558\n",
      "Train Epoch: 9 [25600/60000 (43%)]\tLoss: 0.128679\n",
      "Train Epoch: 9 [26240/60000 (44%)]\tLoss: 0.174299\n",
      "Train Epoch: 9 [26880/60000 (45%)]\tLoss: 0.352681\n",
      "Train Epoch: 9 [27520/60000 (46%)]\tLoss: 0.076442\n",
      "Train Epoch: 9 [28160/60000 (47%)]\tLoss: 0.086913\n",
      "Train Epoch: 9 [28800/60000 (48%)]\tLoss: 0.166126\n",
      "Train Epoch: 9 [29440/60000 (49%)]\tLoss: 0.048320\n",
      "Train Epoch: 9 [30080/60000 (50%)]\tLoss: 0.063871\n",
      "Train Epoch: 9 [30720/60000 (51%)]\tLoss: 0.089976\n",
      "Train Epoch: 9 [31360/60000 (52%)]\tLoss: 0.131959\n",
      "Train Epoch: 9 [32000/60000 (53%)]\tLoss: 0.098003\n",
      "Train Epoch: 9 [32640/60000 (54%)]\tLoss: 0.201005\n",
      "Train Epoch: 9 [33280/60000 (55%)]\tLoss: 0.228544\n",
      "Train Epoch: 9 [33920/60000 (57%)]\tLoss: 0.229336\n",
      "Train Epoch: 9 [34560/60000 (58%)]\tLoss: 0.025956\n",
      "Train Epoch: 9 [35200/60000 (59%)]\tLoss: 0.040246\n",
      "Train Epoch: 9 [35840/60000 (60%)]\tLoss: 0.148997\n",
      "Train Epoch: 9 [36480/60000 (61%)]\tLoss: 0.122416\n",
      "Train Epoch: 9 [37120/60000 (62%)]\tLoss: 0.167470\n",
      "Train Epoch: 9 [37760/60000 (63%)]\tLoss: 0.122178\n",
      "Train Epoch: 9 [38400/60000 (64%)]\tLoss: 0.076769\n",
      "Train Epoch: 9 [39040/60000 (65%)]\tLoss: 0.162754\n",
      "Train Epoch: 9 [39680/60000 (66%)]\tLoss: 0.087576\n",
      "Train Epoch: 9 [40320/60000 (67%)]\tLoss: 0.078068\n",
      "Train Epoch: 9 [40960/60000 (68%)]\tLoss: 0.061370\n",
      "Train Epoch: 9 [41600/60000 (69%)]\tLoss: 0.029429\n",
      "Train Epoch: 9 [42240/60000 (70%)]\tLoss: 0.299636\n",
      "Train Epoch: 9 [42880/60000 (71%)]\tLoss: 0.098215\n",
      "Train Epoch: 9 [43520/60000 (72%)]\tLoss: 0.142153\n",
      "Train Epoch: 9 [44160/60000 (74%)]\tLoss: 0.106360\n",
      "Train Epoch: 9 [44800/60000 (75%)]\tLoss: 0.104826\n",
      "Train Epoch: 9 [45440/60000 (76%)]\tLoss: 0.079051\n",
      "Train Epoch: 9 [46080/60000 (77%)]\tLoss: 0.020090\n",
      "Train Epoch: 9 [46720/60000 (78%)]\tLoss: 0.027074\n",
      "Train Epoch: 9 [47360/60000 (79%)]\tLoss: 0.033409\n",
      "Train Epoch: 9 [48000/60000 (80%)]\tLoss: 0.059375\n",
      "Train Epoch: 9 [48640/60000 (81%)]\tLoss: 0.049688\n",
      "Train Epoch: 9 [49280/60000 (82%)]\tLoss: 0.127529\n",
      "Train Epoch: 9 [49920/60000 (83%)]\tLoss: 0.208519\n",
      "Train Epoch: 9 [50560/60000 (84%)]\tLoss: 0.030237\n",
      "Train Epoch: 9 [51200/60000 (85%)]\tLoss: 0.152231\n",
      "Train Epoch: 9 [51840/60000 (86%)]\tLoss: 0.075549\n",
      "Train Epoch: 9 [52480/60000 (87%)]\tLoss: 0.087543\n",
      "Train Epoch: 9 [53120/60000 (88%)]\tLoss: 0.063019\n",
      "Train Epoch: 9 [53760/60000 (90%)]\tLoss: 0.102955\n",
      "Train Epoch: 9 [54400/60000 (91%)]\tLoss: 0.080291\n",
      "Train Epoch: 9 [55040/60000 (92%)]\tLoss: 0.183937\n",
      "Train Epoch: 9 [55680/60000 (93%)]\tLoss: 0.067712\n",
      "Train Epoch: 9 [56320/60000 (94%)]\tLoss: 0.054795\n",
      "Train Epoch: 9 [56960/60000 (95%)]\tLoss: 0.029141\n",
      "Train Epoch: 9 [57600/60000 (96%)]\tLoss: 0.089555\n",
      "Train Epoch: 9 [58240/60000 (97%)]\tLoss: 0.051685\n",
      "Train Epoch: 9 [58880/60000 (98%)]\tLoss: 0.092712\n",
      "Train Epoch: 9 [59520/60000 (99%)]\tLoss: 0.049173\n",
      "\n",
      "Test set: Average loss: 0.0016, Accuracy: 9692/10000 (96%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.autograd import Variable\n",
    "\n",
    "# Training settings\n",
    "batch_size = 64\n",
    "\n",
    "# MNIST Dataset\n",
    "train_dataset = datasets.MNIST(root='./mnist_data/',\n",
    "                               train=True,\n",
    "                               transform=transforms.ToTensor(),\n",
    "                               download=True)\n",
    "\n",
    "test_dataset = datasets.MNIST(root='./mnist_data/',\n",
    "                              train=False,\n",
    "                              transform=transforms.ToTensor())\n",
    "\n",
    "# Data Loader (Input Pipeline)\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                           batch_size=batch_size,\n",
    "                                           shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
    "                                          batch_size=batch_size,\n",
    "                                          shuffle=False)\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        ...\n",
    "        return\n",
    "\n",
    "    def forward(self, x):\n",
    "        ...\n",
    "        return\n",
    "\n",
    "\n",
    "model = Net()\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.5)\n",
    "\n",
    "\n",
    "def train(epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = Variable(data), Variable(target)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % 10 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.data[0]))\n",
    "\n",
    "\n",
    "def test():\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    for data, target in test_loader:\n",
    "        data, target = Variable(data, volatile=True), Variable(target)\n",
    "        output = model(data)\n",
    "        # sum up batch loss\n",
    "        test_loss += criterion(output, target).data[0]\n",
    "        # get the index of the max\n",
    "        pred = output.data.max(1, keepdim=True)[1]\n",
    "        correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))\n",
    "\n",
    "\n",
    "for epoch in range(1, 10):\n",
    "    train(epoch)\n",
    "    test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='data_loader'></a>\n",
    "# Data Loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, as you might have noticed with the above implementations of the models, we use something called **data loaders**. Data loaders are different tools/pieces of code that enable us to be able to load our data into our model without having to run the computation on all our data points all at once (saving memory and computational load). Data loaders are also not something specific to PyTorch, but the reason why we introduce them today is because data loaders are incredibly intuitive and efficient to use with PyTorch's implementation!\n",
    "\n",
    "When you inherit from PyTorch's data loader class, you are able to inherit from all the different \"workers\" (GPUs), the different techniques of optimization, and random shuffling from the class, enabling you to load in your data more efficiently. But in order for you to use PyTorch's data loaders, you first need to define different methods for the data itself (we don't do that in our models because they're built in MNIST, but in practice, other datasets may need to have a different class/methods defined in order to work properly).\n",
    "\n",
    "Let's take a look at a visual to see more of what we're talking about!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/data-loader.jpg\" width='800'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, in order to actually implement the `train_loader` from above, we need to be able to define an object class that has the following methods:\n",
    "* init\n",
    "* len\n",
    "* getitem\n",
    "\n",
    "That way, when we load this class into PyTorch's dataloader method, we're able to convert the dataset into a generator that can be iterated through to obtain all the different data points. Here's some sudocode of what the different methods and their implemenetation looks like!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class SuperDuperCoolDataset(Dataset):\n",
    "    \"\"\"This is for the SuperDuperCool dataset.\"\"\"\n",
    "    \n",
    "    # Initialize your data, download, etc.\n",
    "    def __init__(self):\n",
    "        ...\n",
    "        return\n",
    "    \n",
    "    # Return one item on the index\n",
    "    def __getitem(self, index):\n",
    "        ...\n",
    "        return\n",
    "    \n",
    "    # Return the data length\n",
    "    def __len__(self):\n",
    "        ...\n",
    "        return\n",
    "    \n",
    "dataset = SuperDuperCoolDataset()\n",
    "train_loader = DataLoader(dataset=dataset,\n",
    "                         batch_size=32,\n",
    "                         shuffle=True,\n",
    "                         num_workers=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='debugging'></a>\n",
    "# Debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-28T05:29:40.493198Z",
     "start_time": "2018-11-28T05:29:40.469896Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from IPython.core.debugger import Pdb\n",
    "debugger = Pdb()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "h(help): Print help\n",
    "\n",
    "n(ext): Continue execution until the next line in the current function is reached or it returns.\n",
    "\n",
    "s(tep): Execute the current line, stop at the first possible occasion (either in a function that is called or in the current function).\n",
    "\n",
    "r(eturn): Continue execution until the current function returns.\n",
    "\n",
    "a(rgs): Print the argument list of the current function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-28T05:29:51.697899Z",
     "start_time": "2018-11-28T05:29:51.691314Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "54"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def my_function(x):\n",
    "    answer = 42\n",
    "    #debugger.set_trace()\n",
    "    answer += x\n",
    "    return answer\n",
    "\n",
    "my_function(12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-28T05:29:11.363593Z",
     "start_time": "2018-11-28T05:29:11.298796Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-213-0d898014faa6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMyModule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;31m#assert y_.shape == (5, 1), y_.shape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'X' is not defined"
     ]
    }
   ],
   "source": [
    "class MyModule(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.lin = nn.Linear(3, 1)\n",
    "    \n",
    "    def forward(self, X):\n",
    "        #set_trace()\n",
    "        x = self.lin(X)\n",
    "        return X\n",
    "\n",
    "    \n",
    "model = MyModule()\n",
    "y = model(X)\n",
    "\n",
    "#assert y_.shape == (5, 1), y_.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='convolutional_neural_network'></a>\n",
    "# Convolutional Neural Network implementation using MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
    "        self.conv2_drop = nn.Dropout2d()\n",
    "        self.fc1 = nn.Linear(320, 50)\n",
    "        self.fc2 = nn.Linear(50, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
    "        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
    "        x = x.view(-1, 320)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "def train(args, model, device, train_loader, optimizer, epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % args.log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))\n",
    "\n",
    "def test(args, model, device, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += F.nll_loss(output, target, reduction='sum').item() # sum up batch loss\n",
    "            pred = output.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='recurrent_neural_networks'></a>\n",
    "# Recurrent Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In other types of neural networks, we make the assumption that all inputs are independent of each other, but many real life situations involve outputs that are dependent of the output before it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='core_ideas'></a>\n",
    "## Core Ideas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Allow us to operate over sequences of vectors (in the input and the output) as opposed to normal feed-forward neural networks and convolutional neural networks which require fixed length vectors.\n",
    "2. Are effective in representing conditional memory - next output is determined from the one before (temporal dependencies)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='applications'></a>\n",
    "## Applications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](img/sequences.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One to many - Image captioning <br>\n",
    "Many to one - Sentiment classification <br>\n",
    "Many to many - Translation (sequence of words -> sequence of words) <br>\n",
    "Many to many (one to one correspondence between input and output) - Video classification on frame level <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='how_rnns_work'></a>\n",
    "# How RNNs Work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='recurrent_neuron'></a>\n",
    "### Recurrent Neuron\n",
    "![title](img/recurrentneuron.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='recurrence_relation'></a>\n",
    "### Recurrence Relation\n",
    "Each \"cell\" has an internal state that gets updated with each input: <br>\n",
    "$$ \\text{RNN cell:} h_t = \\phi(W_R * h_{t-1} + W_I * x_t + b_h)$$ \n",
    " $$ \\text{Linear Layer:} \\hat{y_t} = g_y(W_{y} * h_t + b_y) $$\n",
    "$ h_t $ = new state <br>\n",
    "$ \\phi $ = Some activation function <br>\n",
    "$ W_{R} $ = Matrix of weights connecting hidden layer to hidden layer <br>\n",
    "$ h_{t-1} $ = Previous hidden state <br>\n",
    "$ W_{I} $ = Matrix of weights connecting input layer to hidden layer <br>\n",
    "$ x_t $ = Input vector at time step t <br>\n",
    "$ b_h $ = Bias vector for hidden layer <br>\n",
    "$ \\hat{y_t} $ = Predicted vector for time step t <br>\n",
    "$ y_t $ = Actual vector for time step t <br>\n",
    "$ g_y $ = Function to get normalized probabilities, softmax $(\\frac {e^{f_k}}{\n",
    "\\sum_{j} e^{f_j}})$<br>\n",
    "$ W_{Y} $ = Matrix of weights connecting hidden layer to output layer <br>\n",
    "$ b_y $ = Bias vector for output layer <br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='computation_graph_unrolled'></a>\n",
    "### Computation Graph Unrolled\n",
    "![title](img/unrolled.png) <br>\n",
    "You can think of the hidden state as the the memory of the network. It is calculated based on the previous hidden state and the input at the current step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='backpropagation_through_time'></a>\n",
    "### Backpropagation Through Time\n",
    "![title](img/cost_unrolled.png) <br>\n",
    "We define our cost function to be cross entropy:\n",
    "$$ C(y, \\hat{y}) = - \\sum_{t} y_t \\log{\\hat{y}_t}$$\n",
    "Backpropagation Through Time is the application of backpropagation  but after the network has been unrolled for all input timesteps. The key difference is that we sum up the gradients for the parameter at each time step. In a traditional neural network we dont share parameters across layers, so we dont need to sum anything. <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We proceed by calculating the gradients of the cost with respect to our parameters $W_I$, $W_R$ and $W_y$ <br>\n",
    "Using the cost at time step 2 as an example: <br>\n",
    "Calculating the gradient for $W_y$ only depends on the values at the current time step <br>\n",
    "$$\\frac{\\partial C_2}{\\partial W_y} =  \\frac{\\partial C_2}{\\partial \\hat{y}_2} * \\frac{\\partial \\hat{y}_2}{\\partial h_2} * \\frac{\\partial h_2}{\\partial W_y} $$ <br>\n",
    "We do the same thing with $W_R$:\n",
    "$$ \\frac{\\partial C_2}{\\partial W_R} = \\frac{\\partial C_2}{\\partial \\hat{y}_2} * \\frac{\\partial \\hat{y}_2}{\\partial h_2} * \\frac{\\partial h_2}{\\partial W_R} $$ <br>\n",
    "However, $h_2 = \\phi(W_R * h_1 + W_I * x_t + b_h)$ which depends on $h_1$ and thus $W_R$. Therefore, we cannot treat $h_2$ as a constant. $W_R$ is used in every step of the output, so we need to backpropagate gradients from t=2 all the way to t=0.\n",
    "$$ \\frac{\\partial C_2}{\\partial W_R} = \\sum_{t=0}^2 \\frac{\\partial C_2}{\\partial \\hat{y}_2} * \\frac{\\partial \\hat{y}_2}{\\partial h_2} * \\frac{\\partial h_2}{\\partial h_t} * \\frac{\\partial h_t}{\\partial W_R} $$<br>\n",
    "The same reasoning applies for $W_I$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-22T18:56:42.848327Z",
     "start_time": "2017-12-22T18:56:42.680072Z"
    }
   },
   "source": [
    "<a id='vanishing/exploding_gradient'></a>\n",
    "# Vanishing/Exploding Gradient Problem\n",
    "This idea of propagating all the way to time = 0 introduces the issue of vanishing or exploding gradients. The magnitude of the update for the weight matrix is going to scale with the size of weight matrices:\n",
    "$$ \\frac{\\partial C_T}{\\partial W_R} \\propto |W_R|^T * |\\frac{\\partial \\hat{y}}{\\partial h}|^T $$\n",
    "<br>\n",
    "If the recurrent weight is less than 1, then if you want to calculate the gradient at t=100, it will ne nearly 0. Alternatively, if $W_R$ is greater than 1, you will run into the problem of exploding gradients. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='rnn_extensions'></a>\n",
    "# RNN Extensions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Over the years researchers have developed more sophisticated types of RNNs to deal with some of the shortcomings of the vanilla RNN model.\n",
    "Two of the most popular that deal with the  vanishing/exploding gradient problem while still tracking long-term dependencies effectively are the LSTM and GRU Cells."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='basic_nlp'></a>\n",
    "# Basic NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vocablary - set of unique words in your text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](img/vocabulary.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='representation'></a>\n",
    "## Representation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-28T04:17:03.799834Z",
     "start_time": "2018-11-28T04:17:03.785020Z"
    }
   },
   "source": [
    "We can choose to one hot encode our vectors, however these matrices would be very sparse and have no meaning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](img/onehot.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word embeddings are learned from the corpus we're training on and provide a dense representation of our data with meaning based off of context - the words around it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-28T11:36:49.927708Z",
     "start_time": "2018-11-28T11:36:49.676492Z"
    }
   },
   "source": [
    "![title](img/embeddingvectors.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/vectormath.png\" width=\"500\" height=\"500\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](img/embeddings.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of training your own word embedding layer, you can also use pretrained ones such as Word2Vec and Glove"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='model_dimensions'></a>\n",
    "# Model Dimensions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These our what our batches will look like"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](img/batches.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our input to the RNN will have shape time steps x batch size x embedding and our output will have shape time steps x batch size x vocabulary size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](img/output.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basically our output will represent the probability of the next word from our vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b: batch size\n",
    "\n",
    "e: embedding size\n",
    "\n",
    "v: vocabuary size\n",
    "\n",
    "h: size of hidden state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](img/dimensions.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='shakespeare_generation'></a>\n",
    "# Shakespeare Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-28T03:12:04.899106Z",
     "start_time": "2018-11-28T03:12:04.847964Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "\n",
    "class Dictionary(object):\n",
    "    ''' Represents our Vocabulary.'''\n",
    "    def __init__(self):\n",
    "        self.word2idx = {} #Map of words to indices, convert word into its index in vocabulary\n",
    "        self.idx2word = [] #List of all words, can look up word by index\n",
    "\n",
    "    def add_word(self, word):\n",
    "        if word not in self.word2idx:\n",
    "            self.idx2word.append(word)\n",
    "            self.word2idx[word] = len(self.idx2word) - 1\n",
    "        return self.word2idx[word]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.idx2word)\n",
    "\n",
    "class Corpus(object):\n",
    "    def __init__(self, path):\n",
    "        self.dictionary = Dictionary()\n",
    "        #Converting ASCII for characters we want (limited to those in english language)\n",
    "        self.whitelist = [chr(i) for i in range(32, 127)]\n",
    "        \n",
    "        self.train = self.tokenize(os.path.join(path, 'train.txt'))\n",
    "        self.valid = self.tokenize(os.path.join(path, 'valid.txt'))\n",
    "\n",
    "    def tokenize(self, path):\n",
    "        '''Takes the text file and breaks it up into its individual word.'''\n",
    "        assert os.path.exists(path)\n",
    "        # Add words to the dictionary\n",
    "        with open(path, 'r',  encoding=\"utf8\") as f:\n",
    "            tokens = 0\n",
    "            for line in f:\n",
    "                line = ''.join([c for c in line if c in self.whitelist])\n",
    "                words = line.split() + ['<eos>']\n",
    "                tokens += len(words)\n",
    "                for word in words:\n",
    "                    self.dictionary.add_word(word)\n",
    "\n",
    "        # Tokenize file content\n",
    "        with open(path, 'r',  encoding=\"utf8\") as f:\n",
    "            ids = torch.LongTensor(tokens)\n",
    "            token = 0\n",
    "            for line in f:\n",
    "                line = ''.join([c for c in line if c in self.whitelist])\n",
    "                words = line.split() + ['<eos>']\n",
    "                for word in words:\n",
    "                    ids[token] = self.dictionary.word2idx[word]\n",
    "                    token += 1\n",
    "\n",
    "        return ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-28T03:23:52.325757Z",
     "start_time": "2018-11-28T03:23:31.818032Z"
    }
   },
   "outputs": [],
   "source": [
    "corpus = Corpus('./data/shakespeare')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-28T03:23:57.153186Z",
     "start_time": "2018-11-28T03:23:57.148253Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "famine\n",
      "47\n"
     ]
    }
   ],
   "source": [
    "print(corpus.dictionary.idx2word[47])\n",
    "print(corpus.dictionary.word2idx['famine'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-28T03:24:02.709574Z",
     "start_time": "2018-11-28T03:24:02.704579Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "74010"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Number of unique words\n",
    "vocab_size = len(corpus.dictionary)\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-28T03:24:05.006350Z",
     "start_time": "2018-11-28T03:24:05.001726Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1039900])\n",
      "torch.Size([63420])\n"
     ]
    }
   ],
   "source": [
    "#Number of total words\n",
    "print(corpus.train.size())\n",
    "print(corpus.valid.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-28T10:27:11.366584Z",
     "start_time": "2018-11-28T10:27:11.326745Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "class RNNModel(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, embed_size, hidden_size, num_layers, dropout):\n",
    "        \n",
    "        super(RNNModel, self).__init__()\n",
    "        #Embedding layer going from our vocabulary size to defined embedding size\n",
    "        self.encoder = nn.Embedding(vocab_size, embed_size) \n",
    "        #Add dropout layer to prevent overfitting - randomly zero some neurons\n",
    "        self.drop1 = nn.Dropout(dropout)\n",
    "        self.drop2 = nn.Dropout(dropout)\n",
    "        self.rnn = nn.GRU(embed_size, hidden_size, num_layers, dropout=dropout)\n",
    "        self.decoder = nn.Linear(hidden_size, vocab_size)\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.1\n",
    "        #Sample from Uniform distribution\n",
    "        self.encoder.weight.data.uniform_(-initrange, initrange)\n",
    "        self.decoder.bias.data.fill_(0)\n",
    "        self.decoder.weight.data.uniform_(-initrange, initrange)\n",
    "    \n",
    "    def forward(self, input, hidden):\n",
    "        emb = self.drop1(self.encoder(input))\n",
    "        #Pass in current batch and hidden input from previous timestep\n",
    "        output, hidden = self.rnn(emb, hidden)\n",
    "        output = self.drop2(output)\n",
    "        decoded = self.decoder(output.view(output.size(0)*output.size(1), output.size(2)))\n",
    "        # Reshape output to be time steps x batch size x vocabulary size\n",
    "        return decoded.view(output.size(0), output.size(1), decoded.size(1)), hidden\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        #Gives weights of all the paraemters with everything initialized to 0\n",
    "        weight = next(self.parameters()).data\n",
    "        return Variable(weight.new(self.num_layers, batch_size, self.hidden_size).zero_())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-28T10:27:11.767890Z",
     "start_time": "2018-11-28T10:27:11.761308Z"
    }
   },
   "outputs": [],
   "source": [
    "def batchify(data, batch_size):\n",
    "    # Find out how many batches we have from our corpus\n",
    "    nbatch = data.size(0) // batch_size\n",
    "    # Return narrow version of tensor that has integer mutiple of batch_siz\n",
    "    data = data.narrow(0, 0, nbatch * batch_size)\n",
    "    # Reshape and transpose the data so that it is nbatch x batch_size\n",
    "    data = data.view(batch_size, -1).t().contiguous()\n",
    "    if torch.cuda.is_available():\n",
    "        data = data.cuda()\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-28T10:27:11.984852Z",
     "start_time": "2018-11-28T10:27:11.973979Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Once          a\n",
      "      upon       good\n",
      "         a       king\n",
      "      time        and\n",
      "     there          a\n",
      "       was      queen\n"
     ]
    }
   ],
   "source": [
    "dummy_data = \"Once upon a time there was a good king and a queen\"\n",
    "dummy_data_idx = [corpus.dictionary.word2idx[w] for w in dummy_data.split()]\n",
    "dummy_tensor = torch.LongTensor(dummy_data_idx) \n",
    "op = batchify(dummy_tensor, 2)\n",
    "for row in op:\n",
    "    print(\"%10s %10s\" %  (corpus.dictionary.idx2word[row[0]], corpus.dictionary.idx2word[row[1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-28T10:27:12.265392Z",
     "start_time": "2018-11-28T10:27:12.257022Z"
    }
   },
   "outputs": [],
   "source": [
    "bs_train = 20       # batch size for training set\n",
    "bs_valid = 10       # batch size for validation set\n",
    "bptt_size = 35      # number of times to unroll the graph for back propagation through time\n",
    "clip = 0.25         # gradient clipping to check exploding gradient\n",
    "\n",
    "embed_size = 200    # size of the embedding vector\n",
    "hidden_size = 200   # size of the hidden state in the RNN \n",
    "num_layers = 2      # number of RNN layers to use\n",
    "dropout_pct = 0.5   # %age of neurons to drop out for regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-28T10:27:12.550054Z",
     "start_time": "2018-11-28T10:27:12.523416Z"
    }
   },
   "outputs": [],
   "source": [
    "train_data = batchify(corpus.train, bs_train)\n",
    "val_data = batchify(corpus.valid, bs_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-28T10:27:12.763114Z",
     "start_time": "2018-11-28T10:27:12.754640Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([51995, 20])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#20 sequences of 51995\n",
    "train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-28T10:27:13.608511Z",
     "start_time": "2018-11-28T10:27:12.980524Z"
    }
   },
   "outputs": [],
   "source": [
    "model = RNNModel(vocab_size, embed_size, hidden_size, num_layers, dropout_pct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-28T10:27:13.613137Z",
     "start_time": "2018-11-28T10:27:13.610079Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-28T10:27:13.713950Z",
     "start_time": "2018-11-28T10:27:13.617454Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_batch(source, i):\n",
    "    seq_len = min(bptt_size, len(source) - 1 - i)\n",
    "    data = Variable(source[i:i+seq_len])\n",
    "    target = Variable(source[i+1:i+1+seq_len].view(-1))\n",
    "    if torch.cuda.is_available():\n",
    "        data = data.cuda()\n",
    "        target = target.cuda()\n",
    "    return data, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-28T10:27:13.749528Z",
     "start_time": "2018-11-28T10:27:13.745501Z"
    }
   },
   "outputs": [],
   "source": [
    "data, target = get_batch(train_data, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-28T10:27:13.967695Z",
     "start_time": "2018-11-28T10:27:13.963456Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([35, 20])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#20 sequences of 35 words\n",
    "data.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-28T10:27:14.175125Z",
     "start_time": "2018-11-28T10:27:14.170871Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([700])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Next 20 * 35 words in our corpus\n",
    "target.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-28T10:27:14.815666Z",
     "start_time": "2018-11-28T10:27:14.812790Z"
    }
   },
   "outputs": [],
   "source": [
    "# for sequence in data.t():\n",
    "#     print(\"New Sequence ----\")\n",
    "#     for word in sequence:\n",
    "#         print (corpus.dictionary.idx2word[word])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-28T10:27:15.235324Z",
     "start_time": "2018-11-28T10:27:15.231719Z"
    }
   },
   "outputs": [],
   "source": [
    "# for word in target:\n",
    "#     print(corpus.dictionary.idx2word[word])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-28T10:27:15.565505Z",
     "start_time": "2018-11-28T10:27:15.539600Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(data_source, lr):\n",
    "    # Turn on training mode which enables dropout.\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    #Copy of all the model parameters but initialized to 0\n",
    "    hidden = model.init_hidden(bs_train)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    \n",
    "    for batch, i in enumerate(range(0, data_source.size(0) - 1, bptt_size)):\n",
    "        \n",
    "        data, targets = get_batch(data_source, i)\n",
    "\n",
    "        # This basically \"detaches\" the hidden state from the one from the previous iteration by creating a new Variable\n",
    "        # If we didn't, the model would try backpropagating all the way to start of the dataset.\n",
    "        hidden = Variable(hidden.data)\n",
    "        \n",
    "        if torch.cuda.is_available():\n",
    "            hidden = hidden.cuda()\n",
    "        \n",
    "        # model.zero_grad()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        #Feed in data generated from batch along with hidden state copied over from previous iteration.\n",
    "        output, hidden = model(data, hidden)\n",
    "        loss = criterion(output.view(-1, vocab_size), targets)\n",
    "        loss.backward()\n",
    "\n",
    "        # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
    "        torch.nn.utils.clip_grad_norm(model.parameters(), clip)\n",
    "        \n",
    "        optimizer.step()\n",
    "        total_loss += len(data) * loss.data\n",
    "        \n",
    "    return total_loss[0] / len(data_source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-28T10:27:15.802927Z",
     "start_time": "2018-11-28T10:27:15.787320Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluate(data_source):\n",
    "    # Turn on evaluation mode which disables dropout.\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    hidden = model.init_hidden(bs_valid)\n",
    "    \n",
    "    for i in range(0, data_source.size(0) - 1, bptt_size):\n",
    "        data, targets = get_batch(data_source, i)\n",
    "        \n",
    "        if torch.cuda.is_available():\n",
    "            hidden = hidden.cuda()\n",
    "            \n",
    "        output, hidden = model(data, hidden)\n",
    "        output_flat = output.view(-1, vocab_size)\n",
    "        \n",
    "        total_loss += len(data) * criterion(output_flat, targets).data\n",
    "        hidden = Variable(hidden.data)\n",
    "        \n",
    "    return total_loss[0] / len(data_source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-28T10:27:16.220346Z",
     "start_time": "2018-11-28T10:27:16.216801Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Loop over epochs.\n",
    "best_val_loss = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-28T10:27:16.608232Z",
     "start_time": "2018-11-28T10:27:16.599221Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run(epochs, lr):\n",
    "    global best_val_loss\n",
    "    \n",
    "    for epoch in range(0, epochs):\n",
    "        train_loss = train(train_data, lr)\n",
    "        val_loss = evaluate(val_data)\n",
    "        print(\"Train Loss: \", train_loss, \"Valid Loss: \", val_loss)\n",
    "\n",
    "        if not best_val_loss or val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save(model.state_dict(), \"./4.model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-28T10:27:25.833518Z",
     "start_time": "2018-11-28T10:27:16.984347Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#run(10, 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-28T10:12:07.772629Z",
     "start_time": "2018-11-28T10:12:07.767782Z"
    }
   },
   "outputs": [],
   "source": [
    "#Number of words we want to generate\n",
    "num_words = 200\n",
    "#In the evaluate function, every time a prediction is made the outputs are divided by the \"temperature\" argument \n",
    "#Using a higher number makes all actions more equally likely, and thus gives us \"more random\" outputs.\n",
    "#Using a lower value (less than 1) makes high probabilities contribute more. \n",
    "temperature = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-28T10:12:08.765994Z",
     "start_time": "2018-11-28T10:12:08.242401Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RNNModel(\n",
       "  (encoder): Embedding(74010, 200)\n",
       "  (drop1): Dropout(p=0.5)\n",
       "  (drop2): Dropout(p=0.5)\n",
       "  (rnn): GRU(200, 200, num_layers=2, dropout=0.5)\n",
       "  (decoder): Linear(in_features=200, out_features=74010, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = RNNModel(vocab_size, embed_size, hidden_size, num_layers, dropout_pct)\n",
    "model.load_state_dict(torch.load('./4.model.pth', map_location=lambda storage, loc: storage))\n",
    "if torch.cuda.is_available():\n",
    "    model.cuda()\n",
    "    \n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-28T10:12:10.714476Z",
     "start_time": "2018-11-28T10:12:08.811481Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/calvin/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:3: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To angry senses, \n",
      "And shall. You well, \n",
      "With this highly spread our himself gave him, \n",
      "Your blackness sweet strange; for a rest; \n",
      "You give us bad in an new smiling, \n",
      "coctus! pardon the bow and what fatal parliament \n",
      "And make a purpose. Thou't bring us once to DAUPHIN, \n",
      "The handling to dost comes! \n",
      "Or Caesar to measures sufficit. \n",
      "The gods won; I should be lies to me, \n",
      "His words left on a eye, and La then \n",
      "As as an PEDRO. \n",
      "Your love shall be a night! theres given me us? \n",
      "Exeunt before and and with the BOTTOM. \n",
      "SECOND MURDERER. What was our most matter that of \n",
      "France were a thing by most all short Paris What, of his \n",
      "Hopeless \n",
      "Percy, such it as this husband to you. \n",
      "FALSTAFF. How now, her pagan here for them! Out of mine Cardinal, this need in \n",
      "this come to preserve ground, as that thy wouldst please you about the own man, \n",
      "and seven night for the metal entertain me to cold knowledge or the \n",
      "trod of those was beginning in the pains of this "
     ]
    }
   ],
   "source": [
    "hidden = model.init_hidden(1)\n",
    "idx = corpus.dictionary.word2idx['To']\n",
    "input = Variable(torch.LongTensor([[idx]]).long(), volatile=True)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    input.data = input.data.cuda()\n",
    "    \n",
    "#Print out first word because model will only start printing the next word\n",
    "print(corpus.dictionary.idx2word[idx], '', end='')\n",
    "\n",
    "for i in range(num_words):\n",
    "    output, hidden = model(input, hidden)\n",
    "    word_weights = output.squeeze().data.div(temperature).exp().cpu()\n",
    "    #Creates a Multinomial distribution based off the inputs of word_weights (will automatically normalize to 1)\n",
    "    # and sample from it\n",
    "    word_idx = torch.multinomial(word_weights, 1)[0]\n",
    "    input.data.fill_(word_idx)\n",
    "    word = corpus.dictionary.idx2word[word_idx]\n",
    "\n",
    "    if word == '<eos>':\n",
    "        print('')\n",
    "    else:\n",
    "        print(word + ' ', end='')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Further Reading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
